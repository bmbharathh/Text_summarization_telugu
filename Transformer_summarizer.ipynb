{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_summarizer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGwNaJx1QXjg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "caf93d53-ba1b-4063-db11-e109d00e2be9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zbxhyl_zFlWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import re\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH5cg5pSIHaZ",
        "colab_type": "text"
      },
      "source": [
        "### Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_AjGkWXITKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "news = pd.read_csv(r\"/content/drive/My Drive/telugu - telugu.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXtxc-toIc94",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "c772007a-a1bf-40bf-9eca-669de5ca639c"
      },
      "source": [
        "news.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ప్రమాదకర కరోనా వైరస్‌ భారత ఆర్మీకి సైతం పాకింద...</td>\n",
              "      <td>ఆర్మీ జవాన్‌కు కరోనా పాజిటివ్‌</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>సంక్షోభంలో పడిన  ప్రైవేటు బ్యాంకు యస్‌ బ్యాంకు...</td>\n",
              "      <td>యస్‌’ పునర్నిర్మాణ పథకం, త్వరలోనే ఆంక్షలు ఎత్త...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ఏపీలో స్థానిక సంస్థల ఎన్నికలు వాయిదా వేయడాన్ని...</td>\n",
              "      <td>చంద్రబాబు క్షమాపణ చెప్పాలి</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>పాకిస్తాన్‌ మాజీ క్రికెటర్‌ షోయబ్‌ అక్తర్‌ చైన...</td>\n",
              "      <td>‘ఏది పడితే అది తిని ఈ మహమ్మారిని తెచ్చారు’</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>రాష్ట వ్యాప్తంగా సంచలన సృష్టించిన మిర్యాలగూడ ప...</td>\n",
              "      <td>తల్లి గిరిజను కలిసిన అమృతా ప్రణయ్‌</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text                                            summary\n",
              "0  ప్రమాదకర కరోనా వైరస్‌ భారత ఆర్మీకి సైతం పాకింద...                     ఆర్మీ జవాన్‌కు కరోనా పాజిటివ్‌\n",
              "1  సంక్షోభంలో పడిన  ప్రైవేటు బ్యాంకు యస్‌ బ్యాంకు...  యస్‌’ పునర్నిర్మాణ పథకం, త్వరలోనే ఆంక్షలు ఎత్త...\n",
              "2  ఏపీలో స్థానిక సంస్థల ఎన్నికలు వాయిదా వేయడాన్ని...                         చంద్రబాబు క్షమాపణ చెప్పాలి\n",
              "3  పాకిస్తాన్‌ మాజీ క్రికెటర్‌ షోయబ్‌ అక్తర్‌ చైన...         ‘ఏది పడితే అది తిని ఈ మహమ్మారిని తెచ్చారు’\n",
              "4  రాష్ట వ్యాప్తంగా సంచలన సృష్టించిన మిర్యాలగూడ ప...                 తల్లి గిరిజను కలిసిన అమృతా ప్రణయ్‌"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vR2hg9themaN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2571618b-33fb-4c49-d2b3-89632e872dd3"
      },
      "source": [
        "news.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1007, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04oeUTvbQ4qY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "document = news['text']\n",
        "summary = news['summary']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2z55AhpKIdK7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "2bb116af-1bc6-4634-d094-2b2cd15ac768"
      },
      "source": [
        "document[30], summary[30]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('ప్రపంచమార్కెట్లలో కరోనా ప్రళయం కొనసాగుతూనే ఉంది. మహా పతనాల బాటలో స్టాక్\\u200cమార్కెట్లు శుక్రవారం కూడా మరింత అధఃపాతాళానికి పడిపోయాయి. భారత్\\u200cలో తొలి కరోనా మరణం నమోదు కావడంతో మార్కెట్\\u200c మరోమారు అత్యంత ఘోరంగా కుప్పకూలింది. అయితే, అంతేవేగంతో నేలక్కొట్టిన బంతిలా మార్కెట్\\u200c దూసుకెళ్లి ఇన్వెస్టర్లకు అసలుసిసలు రోలర్\\u200c కోస్టర్\\u200c రైడ్\\u200cను చూపించింది. గడిచిన 12 ఏళ్లలో ఎన్నడూ జరగని రీతిలో తొలిసారి మన స్టాక్\\u200c మార్కెట్లో మళ్లీ ట్రేడింగ్\\u200c నిలిపేయాల్సిన పరిస్థితి తలెత్తింది.ఒకానొక దశలో సెన్సెక్స్\\u200c 3,389 పాయింట్లు నష్టపోయి... ఆ కనిష్ట స్థాయి నుంచి 5,380 పాయింట్లు దూసుకెళ్లడం తీవ్రమైన ఒడిదుడుకులకు నిదర్శనం. చివరకు 1,325 పాయింట్లు లాభపడి 34,103 వద్ద ముగిసింది. ఒకేరోజు ఇంత ఘోరంగా పడిపోవడం, మళ్లీ ఈస్థాయిలో రికవరీ.. ఈ రెండూ కూడా కొత్త రికార్డులే కావడం గమనార్హం. కాగా, శుక్రవారం ఆరంభంలో 15 నిమిషాల్లోనే రూ.12.9 లక్షల కోట్ల ఇన్వెస్టర్ల సంపద తుడిచిపెట్టుకుపోగా... చివరికి ఈ నష్టాలన్నింటినీ పూడ్చుకోవడంతోపాటు రూ.3.5 లక్షల కోట్ల మార్కెట్\\u200c విలువ పెరగడం విశేషం!! ',\n",
              " '‘కోవిడ్\\u200c’ కోస్టర్\\u200c..!45 నిమిషాల పాటు ట్రేడింగ్\\u200c నిలిపివేత,సెన్సెక్స్, నిఫ్టీలు ఆరంభంలోనే 10 శాతం డౌన్\\u200c,ఇంట్రాడేలో 3,389 పాయింట్లు పతనమైన సెన్సెక్స్\\u200c,1,325 పాయింట్ల లాభంతో 34,103 వద్ద ముగింపు ')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8gKyq1gIq4r",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJ6LE4MrJjC_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "30b724e7-c0e6-425f-d0a1-1cd8a90bfc70"
      },
      "source": [
        "# for decoder sequence\n",
        "summary = summary.apply(lambda x: '<go> ' + str(x) + ' <stop>')\n",
        "summary.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0           <go> ఆర్మీ జవాన్‌కు కరోనా పాజిటివ్‌ <stop>\n",
              "1    <go> యస్‌’ పునర్నిర్మాణ పథకం, త్వరలోనే ఆంక్షలు...\n",
              "2               <go> చంద్రబాబు క్షమాపణ చెప్పాలి <stop>\n",
              "3    <go> ‘ఏది పడితే అది తిని ఈ మహమ్మారిని తెచ్చారు...\n",
              "4       <go> తల్లి గిరిజను కలిసిన అమృతా ప్రణయ్‌ <stop>\n",
              "Name: summary, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95Zv7FIvKbTi",
        "colab_type": "text"
      },
      "source": [
        "#### Tokenizing the texts into integer tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TqbpEyPMRqa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# since < and > from default tokens cannot be removed\n",
        "filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n",
        "oov_token = '<unk>'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHw2csoYImsa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "document_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token)\n",
        "summary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWU9Xu7OKVab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "document_tokenizer.fit_on_texts(document)\n",
        "summary_tokenizer.fit_on_texts(summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ESm-aYR-tvx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = document_tokenizer.texts_to_sequences(document)\n",
        "targets = summary_tokenizer.texts_to_sequences(summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVyErXAei5_b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 966
        },
        "outputId": "55eeb0c1-b1e7-435c-c888-f8e0069b3a8e"
      },
      "source": [
        "summary_tokenizer.texts_to_sequences([\"దేశంలో జరిగే అన్ని ఫుట్‌బాల్‌ మ్యాచ్‌లను ఈ నెల 31 వరకు రద్దు చేస్తూ అఖిల భారత ఫుట్‌బాల్‌ సమాఖ్య (ఏఐఎఫ్‌ఎఫ్‌) శనివారం నిర్ణయం తీసుకుంది. దాంతో ఐ–లీగ్, డివిజన్‌–2, యూత్‌ లీగ్, గోల్డెన్‌ లీగ్, జాతీయ టోర్నీలు రద్దయ్యాయి. ఐ–లీగ్‌లోని 28 మ్యాచ్‌లను ప్రేక్షకులు లేకుండానే నిర్వహించాలని ఏఐఎఫ్‌ఎఫ్‌ తొలుత అనుకున్నా... కేంద్ర ఆరోగ్య మంత్రిత్వ శాఖ సలహా మేరకు ఈ నెల చివరి వరకు దేశంలో ఎటువంటి ఫుట్‌బాల్‌ మ్యాచ్‌లను నిర్వహించరాదని  నిర్ణయించింది.\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[656,\n",
              "  1,\n",
              "  2158,\n",
              "  76,\n",
              "  1,\n",
              "  12,\n",
              "  204,\n",
              "  75,\n",
              "  48,\n",
              "  283,\n",
              "  1,\n",
              "  1,\n",
              "  105,\n",
              "  76,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1573,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  387,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  467,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  286,\n",
              "  1,\n",
              "  1,\n",
              "  954,\n",
              "  1,\n",
              "  1,\n",
              "  12,\n",
              "  204,\n",
              "  968,\n",
              "  48,\n",
              "  656,\n",
              "  1,\n",
              "  76,\n",
              "  1,\n",
              "  1,\n",
              "  1]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ryx9qx90jwXu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bc4c2598-0f42-4b64-ac13-45c9cbdb25bd"
      },
      "source": [
        "summary_tokenizer.sequences_to_texts([[184, 22, 12, 71]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ఘోర చంద్రబాబు ఈ రియల్టీ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoizyBvLKv8h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "555ee9b6-7e70-4c85-b1a9-b7dbbe73bc1b"
      },
      "source": [
        "encoder_vocab_size = len(document_tokenizer.word_index) + 1\n",
        "decoder_vocab_size = len(summary_tokenizer.word_index) + 1\n",
        "\n",
        "# vocab_size\n",
        "encoder_vocab_size, decoder_vocab_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25646, 2938)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZden_q9_eZr",
        "colab_type": "text"
      },
      "source": [
        "#### Obtaining insights on lengths for defining maxlen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ma4o2nGdK5Xb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "document_lengths = pd.Series([len(x) for x in document])\n",
        "summary_lengths = pd.Series([len(x) for x in summary])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXZlO99C-UXK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "baf99805-f357-4249-a850-ea3b8e19503a"
      },
      "source": [
        "document_lengths.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    1007.000000\n",
              "mean      634.841112\n",
              "std       206.310924\n",
              "min       208.000000\n",
              "25%       478.500000\n",
              "50%       618.000000\n",
              "75%       747.000000\n",
              "max      1805.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALMwKMx--ZF7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "74c2b333-9f54-4d3a-c987-54c112f59ee9"
      },
      "source": [
        "summary_lengths.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    1007.000000\n",
              "mean       44.391261\n",
              "std        13.294636\n",
              "min        20.000000\n",
              "25%        37.000000\n",
              "50%        43.000000\n",
              "75%        50.000000\n",
              "max       203.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVeMilXr-bpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# maxlen\n",
        "# taking values > and round figured to 75th percentile\n",
        "# at the same time not leaving high variance\n",
        "encoder_maxlen = 400\n",
        "decoder_maxlen = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SWap3YJBk-D",
        "colab_type": "text"
      },
      "source": [
        "#### Padding/Truncating sequences for identical sequence lengths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEyUBeu7ACRt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=decoder_maxlen, padding='post', truncating='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIP0kIIcB8Rm",
        "colab_type": "text"
      },
      "source": [
        "### Creating dataset pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzO6l3-AB7hJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = tf.cast(inputs, dtype=tf.int32)\n",
        "targets = tf.cast(targets, dtype=tf.int32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slZ5f4P4DurS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = 2000\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI-fV7eABWN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isN1CpAXLfsl",
        "colab_type": "text"
      },
      "source": [
        "### Positional Encoding for adding notion of position among words as unlike RNN this is non-directional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Purv7oyhETDZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_angles(position, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return position * angle_rates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40J2pc2NEXp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(\n",
        "        np.arange(position)[:, np.newaxis],\n",
        "        np.arange(d_model)[np.newaxis, :],\n",
        "        d_model\n",
        "    )\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24Pe01DMMWHc",
        "colab_type": "text"
      },
      "source": [
        "### Masking\n",
        "\n",
        "- Padding mask for masking \"pad\" sequences\n",
        "- Lookahead mask for masking future words from contributing in prediction of current words in self attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN1wVQAdMVYy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmjAPLWuMREE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8DqUBc4NFOy",
        "colab_type": "text"
      },
      "source": [
        "### Building the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfknVF7hNKf7",
        "colab_type": "text"
      },
      "source": [
        "#### Scaled Dot Product"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_B6M9OBNBKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "    return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf7_a5uQOfJk",
        "colab_type": "text"
      },
      "source": [
        "#### Multi-Headed Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIuFrdXnNZEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
        "        output = self.dense(concat_attention)\n",
        "            \n",
        "        return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A49tXMVvOkOZ",
        "colab_type": "text"
      },
      "source": [
        "### Feed Forward Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9-qoKuTNwKq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),\n",
        "        tf.keras.layers.Dense(d_model)\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2RRmn2bOpW9",
        "colab_type": "text"
      },
      "source": [
        "#### Fundamental Unit of Transformer encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNuoJoFWO335",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i6Zh8gnPqdW",
        "colab_type": "text"
      },
      "source": [
        "#### Fundamental Unit of Transformer decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CVmvs6dPMRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zt5MUc_QNid",
        "colab_type": "text"
      },
      "source": [
        "#### Encoder consisting of multiple EncoderLayer(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrbnTwijQJ-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "    def call(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "    \n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "    \n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4N5LrNrvRexg",
        "colab_type": "text"
      },
      "source": [
        "#### Decoder consisting of multiple DecoderLayer(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmeqkZrIRbSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "    \n",
        "        return x, attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbMNK_bzSHnh",
        "colab_type": "text"
      },
      "source": [
        "#### Finally, the Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXHRG-o4R9Mc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
        "\n",
        "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)\n",
        "\n",
        "        return final_output, attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UndsMPZXTdSr",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQPBv8FWTg1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hyper-params\n",
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 4\n",
        "EPOCHS = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOGvkYDNTjIj",
        "colab_type": "text"
      },
      "source": [
        "#### Adam optimizer with custom learning rate scheduling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfiynCLlTL8C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsVdrENTUERY",
        "colab_type": "text"
      },
      "source": [
        "#### Defining losses and other metrics "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ip1-943kTXXK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktKwyvKtTvF6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW4LA_45T4Aa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ze0u6xxXT7dI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XvKy3v6ULnO",
        "colab_type": "text"
      },
      "source": [
        "#### Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5-RcxqFUCuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer = Transformer(\n",
        "    num_layers, \n",
        "    d_model, \n",
        "    num_heads, \n",
        "    dff,\n",
        "    encoder_vocab_size, \n",
        "    decoder_vocab_size, \n",
        "    pe_input=encoder_vocab_size, \n",
        "    pe_target=decoder_vocab_size,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f56BGiVXU_Dk",
        "colab_type": "text"
      },
      "source": [
        "#### Masks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZxHuyZxU5Pa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  \n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYIotvaBVI0d",
        "colab_type": "text"
      },
      "source": [
        "#### Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOc1_3c-VGaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = \"/content/sample_data/checkpoints\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print ('Latest checkpoint restored!!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfpI0gS4c06c",
        "colab_type": "text"
      },
      "source": [
        "#### Training steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmVOMzkrczgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(\n",
        "            inp, tar_inp, \n",
        "            True, \n",
        "            enc_padding_mask, \n",
        "            combined_mask, \n",
        "            dec_padding_mask\n",
        "        )\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5Ms2P6NWxok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "los = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhZqo4bNW4Zh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "63fece69-2ddd-44fa-b937-5be83aa2a11d"
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "  \n",
        "    for (batch, (inp, tar)) in enumerate(dataset):\n",
        "        train_step(inp, tar)\n",
        "    \n",
        "        # 55k samples\n",
        "        # we display 3 batch results -- 0th, middle and last one (approx)\n",
        "        # 55k / 64 ~ 858; 858 / 2 = 429\n",
        "        #if batch % 429 == 0:\n",
        "        print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, train_loss.result()))\n",
        "      \n",
        "    if (epoch + 1) % 1:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
        "    \n",
        "    print ('Epoch {} Loss {:.4f}'.format(epoch + 1, train_loss.result()))\n",
        "    los.append(train_loss.result())\n",
        "\n",
        "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 8.0477\n",
            "Epoch 1 Batch 1 Loss 8.0374\n",
            "Epoch 1 Batch 2 Loss 8.0363\n",
            "Epoch 1 Batch 3 Loss 8.0388\n",
            "Epoch 1 Batch 4 Loss 8.0427\n",
            "Epoch 1 Batch 5 Loss 8.0436\n",
            "Epoch 1 Batch 6 Loss 8.0416\n",
            "Epoch 1 Batch 7 Loss 8.0423\n",
            "Epoch 1 Batch 8 Loss 8.0410\n",
            "Epoch 1 Batch 9 Loss 8.0381\n",
            "Epoch 1 Batch 10 Loss 8.0370\n",
            "Epoch 1 Batch 11 Loss 8.0362\n",
            "Epoch 1 Batch 12 Loss 8.0355\n",
            "Epoch 1 Batch 13 Loss 8.0332\n",
            "Epoch 1 Batch 14 Loss 8.0299\n",
            "Epoch 1 Batch 15 Loss 8.0290\n",
            "Epoch 1 Loss 8.0290\n",
            "Time taken for 1 epoch: 17.205416202545166 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 7.9843\n",
            "Epoch 2 Batch 1 Loss 7.9735\n",
            "Epoch 2 Batch 2 Loss 7.9661\n",
            "Epoch 2 Batch 3 Loss 7.9693\n",
            "Epoch 2 Batch 4 Loss 7.9689\n",
            "Epoch 2 Batch 5 Loss 7.9665\n",
            "Epoch 2 Batch 6 Loss 7.9623\n",
            "Epoch 2 Batch 7 Loss 7.9575\n",
            "Epoch 2 Batch 8 Loss 7.9548\n",
            "Epoch 2 Batch 9 Loss 7.9504\n",
            "Epoch 2 Batch 10 Loss 7.9473\n",
            "Epoch 2 Batch 11 Loss 7.9437\n",
            "Epoch 2 Batch 12 Loss 7.9399\n",
            "Epoch 2 Batch 13 Loss 7.9354\n",
            "Epoch 2 Batch 14 Loss 7.9323\n",
            "Epoch 2 Batch 15 Loss 7.9277\n",
            "Epoch 2 Loss 7.9277\n",
            "Time taken for 1 epoch: 3.4825804233551025 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 7.8756\n",
            "Epoch 3 Batch 1 Loss 7.8485\n",
            "Epoch 3 Batch 2 Loss 7.8468\n",
            "Epoch 3 Batch 3 Loss 7.8383\n",
            "Epoch 3 Batch 4 Loss 7.8313\n",
            "Epoch 3 Batch 5 Loss 7.8245\n",
            "Epoch 3 Batch 6 Loss 7.8167\n",
            "Epoch 3 Batch 7 Loss 7.8140\n",
            "Epoch 3 Batch 8 Loss 7.8102\n",
            "Epoch 3 Batch 9 Loss 7.8038\n",
            "Epoch 3 Batch 10 Loss 7.7979\n",
            "Epoch 3 Batch 11 Loss 7.7913\n",
            "Epoch 3 Batch 12 Loss 7.7883\n",
            "Epoch 3 Batch 13 Loss 7.7841\n",
            "Epoch 3 Batch 14 Loss 7.7794\n",
            "Epoch 3 Batch 15 Loss 7.7748\n",
            "Epoch 3 Loss 7.7748\n",
            "Time taken for 1 epoch: 3.477698802947998 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 7.6958\n",
            "Epoch 4 Batch 1 Loss 7.6740\n",
            "Epoch 4 Batch 2 Loss 7.6788\n",
            "Epoch 4 Batch 3 Loss 7.6802\n",
            "Epoch 4 Batch 4 Loss 7.6792\n",
            "Epoch 4 Batch 5 Loss 7.6749\n",
            "Epoch 4 Batch 6 Loss 7.6705\n",
            "Epoch 4 Batch 7 Loss 7.6697\n",
            "Epoch 4 Batch 8 Loss 7.6683\n",
            "Epoch 4 Batch 9 Loss 7.6669\n",
            "Epoch 4 Batch 10 Loss 7.6631\n",
            "Epoch 4 Batch 11 Loss 7.6573\n",
            "Epoch 4 Batch 12 Loss 7.6520\n",
            "Epoch 4 Batch 13 Loss 7.6472\n",
            "Epoch 4 Batch 14 Loss 7.6468\n",
            "Epoch 4 Batch 15 Loss 7.6436\n",
            "Epoch 4 Loss 7.6436\n",
            "Time taken for 1 epoch: 3.470977783203125 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 7.5803\n",
            "Epoch 5 Batch 1 Loss 7.6034\n",
            "Epoch 5 Batch 2 Loss 7.5920\n",
            "Epoch 5 Batch 3 Loss 7.5912\n",
            "Epoch 5 Batch 4 Loss 7.5841\n",
            "Epoch 5 Batch 5 Loss 7.5796\n",
            "Epoch 5 Batch 6 Loss 7.5781\n",
            "Epoch 5 Batch 7 Loss 7.5731\n",
            "Epoch 5 Batch 8 Loss 7.5715\n",
            "Epoch 5 Batch 9 Loss 7.5639\n",
            "Epoch 5 Batch 10 Loss 7.5608\n",
            "Epoch 5 Batch 11 Loss 7.5572\n",
            "Epoch 5 Batch 12 Loss 7.5575\n",
            "Epoch 5 Batch 13 Loss 7.5546\n",
            "Epoch 5 Batch 14 Loss 7.5533\n",
            "Epoch 5 Batch 15 Loss 7.5493\n",
            "Epoch 5 Loss 7.5493\n",
            "Time taken for 1 epoch: 3.4667270183563232 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 7.5002\n",
            "Epoch 6 Batch 1 Loss 7.4964\n",
            "Epoch 6 Batch 2 Loss 7.5141\n",
            "Epoch 6 Batch 3 Loss 7.5054\n",
            "Epoch 6 Batch 4 Loss 7.4947\n",
            "Epoch 6 Batch 5 Loss 7.4946\n",
            "Epoch 6 Batch 6 Loss 7.4949\n",
            "Epoch 6 Batch 7 Loss 7.4988\n",
            "Epoch 6 Batch 8 Loss 7.4990\n",
            "Epoch 6 Batch 9 Loss 7.4933\n",
            "Epoch 6 Batch 10 Loss 7.4896\n",
            "Epoch 6 Batch 11 Loss 7.4938\n",
            "Epoch 6 Batch 12 Loss 7.4906\n",
            "Epoch 6 Batch 13 Loss 7.4889\n",
            "Epoch 6 Batch 14 Loss 7.4878\n",
            "Epoch 6 Batch 15 Loss 7.4868\n",
            "Epoch 6 Loss 7.4868\n",
            "Time taken for 1 epoch: 3.4546008110046387 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 7.4705\n",
            "Epoch 7 Batch 1 Loss 7.4638\n",
            "Epoch 7 Batch 2 Loss 7.4372\n",
            "Epoch 7 Batch 3 Loss 7.4429\n",
            "Epoch 7 Batch 4 Loss 7.4562\n",
            "Epoch 7 Batch 5 Loss 7.4559\n",
            "Epoch 7 Batch 6 Loss 7.4551\n",
            "Epoch 7 Batch 7 Loss 7.4513\n",
            "Epoch 7 Batch 8 Loss 7.4504\n",
            "Epoch 7 Batch 9 Loss 7.4485\n",
            "Epoch 7 Batch 10 Loss 7.4471\n",
            "Epoch 7 Batch 11 Loss 7.4503\n",
            "Epoch 7 Batch 12 Loss 7.4532\n",
            "Epoch 7 Batch 13 Loss 7.4483\n",
            "Epoch 7 Batch 14 Loss 7.4455\n",
            "Epoch 7 Batch 15 Loss 7.4434\n",
            "Epoch 7 Loss 7.4434\n",
            "Time taken for 1 epoch: 3.469385862350464 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 7.4475\n",
            "Epoch 8 Batch 1 Loss 7.4096\n",
            "Epoch 8 Batch 2 Loss 7.4180\n",
            "Epoch 8 Batch 3 Loss 7.4239\n",
            "Epoch 8 Batch 4 Loss 7.4238\n",
            "Epoch 8 Batch 5 Loss 7.4262\n",
            "Epoch 8 Batch 6 Loss 7.4220\n",
            "Epoch 8 Batch 7 Loss 7.4099\n",
            "Epoch 8 Batch 8 Loss 7.4106\n",
            "Epoch 8 Batch 9 Loss 7.4145\n",
            "Epoch 8 Batch 10 Loss 7.4110\n",
            "Epoch 8 Batch 11 Loss 7.4112\n",
            "Epoch 8 Batch 12 Loss 7.4088\n",
            "Epoch 8 Batch 13 Loss 7.4065\n",
            "Epoch 8 Batch 14 Loss 7.4068\n",
            "Epoch 8 Batch 15 Loss 7.4077\n",
            "Epoch 8 Loss 7.4077\n",
            "Time taken for 1 epoch: 3.451991558074951 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 7.4024\n",
            "Epoch 9 Batch 1 Loss 7.3833\n",
            "Epoch 9 Batch 2 Loss 7.3731\n",
            "Epoch 9 Batch 3 Loss 7.3853\n",
            "Epoch 9 Batch 4 Loss 7.3762\n",
            "Epoch 9 Batch 5 Loss 7.3806\n",
            "Epoch 9 Batch 6 Loss 7.3792\n",
            "Epoch 9 Batch 7 Loss 7.3733\n",
            "Epoch 9 Batch 8 Loss 7.3738\n",
            "Epoch 9 Batch 9 Loss 7.3752\n",
            "Epoch 9 Batch 10 Loss 7.3787\n",
            "Epoch 9 Batch 11 Loss 7.3786\n",
            "Epoch 9 Batch 12 Loss 7.3717\n",
            "Epoch 9 Batch 13 Loss 7.3707\n",
            "Epoch 9 Batch 14 Loss 7.3713\n",
            "Epoch 9 Batch 15 Loss 7.3705\n",
            "Epoch 9 Loss 7.3705\n",
            "Time taken for 1 epoch: 3.462825298309326 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 7.3871\n",
            "Epoch 10 Batch 1 Loss 7.3697\n",
            "Epoch 10 Batch 2 Loss 7.3673\n",
            "Epoch 10 Batch 3 Loss 7.3538\n",
            "Epoch 10 Batch 4 Loss 7.3560\n",
            "Epoch 10 Batch 5 Loss 7.3590\n",
            "Epoch 10 Batch 6 Loss 7.3475\n",
            "Epoch 10 Batch 7 Loss 7.3462\n",
            "Epoch 10 Batch 8 Loss 7.3476\n",
            "Epoch 10 Batch 9 Loss 7.3423\n",
            "Epoch 10 Batch 10 Loss 7.3420\n",
            "Epoch 10 Batch 11 Loss 7.3433\n",
            "Epoch 10 Batch 12 Loss 7.3453\n",
            "Epoch 10 Batch 13 Loss 7.3407\n",
            "Epoch 10 Batch 14 Loss 7.3409\n",
            "Epoch 10 Batch 15 Loss 7.3413\n",
            "Epoch 10 Loss 7.3413\n",
            "Time taken for 1 epoch: 3.4650092124938965 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 7.3528\n",
            "Epoch 11 Batch 1 Loss 7.3238\n",
            "Epoch 11 Batch 2 Loss 7.3194\n",
            "Epoch 11 Batch 3 Loss 7.3157\n",
            "Epoch 11 Batch 4 Loss 7.3305\n",
            "Epoch 11 Batch 5 Loss 7.3290\n",
            "Epoch 11 Batch 6 Loss 7.3280\n",
            "Epoch 11 Batch 7 Loss 7.3279\n",
            "Epoch 11 Batch 8 Loss 7.3234\n",
            "Epoch 11 Batch 9 Loss 7.3191\n",
            "Epoch 11 Batch 10 Loss 7.3158\n",
            "Epoch 11 Batch 11 Loss 7.3131\n",
            "Epoch 11 Batch 12 Loss 7.3108\n",
            "Epoch 11 Batch 13 Loss 7.3082\n",
            "Epoch 11 Batch 14 Loss 7.3085\n",
            "Epoch 11 Batch 15 Loss 7.3038\n",
            "Epoch 11 Loss 7.3038\n",
            "Time taken for 1 epoch: 3.4787039756774902 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 7.2823\n",
            "Epoch 12 Batch 1 Loss 7.2740\n",
            "Epoch 12 Batch 2 Loss 7.2719\n",
            "Epoch 12 Batch 3 Loss 7.2685\n",
            "Epoch 12 Batch 4 Loss 7.2619\n",
            "Epoch 12 Batch 5 Loss 7.2650\n",
            "Epoch 12 Batch 6 Loss 7.2656\n",
            "Epoch 12 Batch 7 Loss 7.2660\n",
            "Epoch 12 Batch 8 Loss 7.2685\n",
            "Epoch 12 Batch 9 Loss 7.2652\n",
            "Epoch 12 Batch 10 Loss 7.2626\n",
            "Epoch 12 Batch 11 Loss 7.2635\n",
            "Epoch 12 Batch 12 Loss 7.2620\n",
            "Epoch 12 Batch 13 Loss 7.2612\n",
            "Epoch 12 Batch 14 Loss 7.2647\n",
            "Epoch 12 Batch 15 Loss 7.2666\n",
            "Epoch 12 Loss 7.2666\n",
            "Time taken for 1 epoch: 3.4756953716278076 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 7.2720\n",
            "Epoch 13 Batch 1 Loss 7.2591\n",
            "Epoch 13 Batch 2 Loss 7.2443\n",
            "Epoch 13 Batch 3 Loss 7.2375\n",
            "Epoch 13 Batch 4 Loss 7.2262\n",
            "Epoch 13 Batch 5 Loss 7.2204\n",
            "Epoch 13 Batch 6 Loss 7.2270\n",
            "Epoch 13 Batch 7 Loss 7.2280\n",
            "Epoch 13 Batch 8 Loss 7.2252\n",
            "Epoch 13 Batch 9 Loss 7.2312\n",
            "Epoch 13 Batch 10 Loss 7.2331\n",
            "Epoch 13 Batch 11 Loss 7.2296\n",
            "Epoch 13 Batch 12 Loss 7.2300\n",
            "Epoch 13 Batch 13 Loss 7.2244\n",
            "Epoch 13 Batch 14 Loss 7.2278\n",
            "Epoch 13 Batch 15 Loss 7.2244\n",
            "Epoch 13 Loss 7.2244\n",
            "Time taken for 1 epoch: 3.4873948097229004 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 7.1307\n",
            "Epoch 14 Batch 1 Loss 7.1575\n",
            "Epoch 14 Batch 2 Loss 7.1707\n",
            "Epoch 14 Batch 3 Loss 7.1756\n",
            "Epoch 14 Batch 4 Loss 7.1867\n",
            "Epoch 14 Batch 5 Loss 7.1880\n",
            "Epoch 14 Batch 6 Loss 7.1859\n",
            "Epoch 14 Batch 7 Loss 7.1830\n",
            "Epoch 14 Batch 8 Loss 7.1840\n",
            "Epoch 14 Batch 9 Loss 7.1811\n",
            "Epoch 14 Batch 10 Loss 7.1796\n",
            "Epoch 14 Batch 11 Loss 7.1775\n",
            "Epoch 14 Batch 12 Loss 7.1781\n",
            "Epoch 14 Batch 13 Loss 7.1789\n",
            "Epoch 14 Batch 14 Loss 7.1831\n",
            "Epoch 14 Batch 15 Loss 7.1823\n",
            "Epoch 14 Loss 7.1823\n",
            "Time taken for 1 epoch: 3.499767541885376 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 7.1255\n",
            "Epoch 15 Batch 1 Loss 7.1728\n",
            "Epoch 15 Batch 2 Loss 7.1717\n",
            "Epoch 15 Batch 3 Loss 7.1625\n",
            "Epoch 15 Batch 4 Loss 7.1512\n",
            "Epoch 15 Batch 5 Loss 7.1600\n",
            "Epoch 15 Batch 6 Loss 7.1552\n",
            "Epoch 15 Batch 7 Loss 7.1484\n",
            "Epoch 15 Batch 8 Loss 7.1468\n",
            "Epoch 15 Batch 9 Loss 7.1440\n",
            "Epoch 15 Batch 10 Loss 7.1454\n",
            "Epoch 15 Batch 11 Loss 7.1451\n",
            "Epoch 15 Batch 12 Loss 7.1396\n",
            "Epoch 15 Batch 13 Loss 7.1405\n",
            "Epoch 15 Batch 14 Loss 7.1397\n",
            "Epoch 15 Batch 15 Loss 7.1354\n",
            "Epoch 15 Loss 7.1354\n",
            "Time taken for 1 epoch: 3.519911289215088 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 7.1130\n",
            "Epoch 16 Batch 1 Loss 7.0691\n",
            "Epoch 16 Batch 2 Loss 7.0679\n",
            "Epoch 16 Batch 3 Loss 7.0914\n",
            "Epoch 16 Batch 4 Loss 7.0815\n",
            "Epoch 16 Batch 5 Loss 7.0956\n",
            "Epoch 16 Batch 6 Loss 7.0992\n",
            "Epoch 16 Batch 7 Loss 7.0963\n",
            "Epoch 16 Batch 8 Loss 7.0938\n",
            "Epoch 16 Batch 9 Loss 7.0884\n",
            "Epoch 16 Batch 10 Loss 7.0866\n",
            "Epoch 16 Batch 11 Loss 7.0853\n",
            "Epoch 16 Batch 12 Loss 7.0834\n",
            "Epoch 16 Batch 13 Loss 7.0825\n",
            "Epoch 16 Batch 14 Loss 7.0868\n",
            "Epoch 16 Batch 15 Loss 7.0867\n",
            "Epoch 16 Loss 7.0867\n",
            "Time taken for 1 epoch: 3.4760730266571045 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 7.0890\n",
            "Epoch 17 Batch 1 Loss 7.0481\n",
            "Epoch 17 Batch 2 Loss 7.0577\n",
            "Epoch 17 Batch 3 Loss 7.0546\n",
            "Epoch 17 Batch 4 Loss 7.0565\n",
            "Epoch 17 Batch 5 Loss 7.0519\n",
            "Epoch 17 Batch 6 Loss 7.0474\n",
            "Epoch 17 Batch 7 Loss 7.0498\n",
            "Epoch 17 Batch 8 Loss 7.0453\n",
            "Epoch 17 Batch 9 Loss 7.0480\n",
            "Epoch 17 Batch 10 Loss 7.0459\n",
            "Epoch 17 Batch 11 Loss 7.0423\n",
            "Epoch 17 Batch 12 Loss 7.0365\n",
            "Epoch 17 Batch 13 Loss 7.0389\n",
            "Epoch 17 Batch 14 Loss 7.0384\n",
            "Epoch 17 Batch 15 Loss 7.0368\n",
            "Epoch 17 Loss 7.0368\n",
            "Time taken for 1 epoch: 3.50109601020813 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 6.9521\n",
            "Epoch 18 Batch 1 Loss 6.9922\n",
            "Epoch 18 Batch 2 Loss 7.0035\n",
            "Epoch 18 Batch 3 Loss 6.9909\n",
            "Epoch 18 Batch 4 Loss 6.9965\n",
            "Epoch 18 Batch 5 Loss 7.0114\n",
            "Epoch 18 Batch 6 Loss 6.9985\n",
            "Epoch 18 Batch 7 Loss 6.9963\n",
            "Epoch 18 Batch 8 Loss 7.0002\n",
            "Epoch 18 Batch 9 Loss 6.9954\n",
            "Epoch 18 Batch 10 Loss 6.9890\n",
            "Epoch 18 Batch 11 Loss 6.9864\n",
            "Epoch 18 Batch 12 Loss 6.9837\n",
            "Epoch 18 Batch 13 Loss 6.9862\n",
            "Epoch 18 Batch 14 Loss 6.9868\n",
            "Epoch 18 Batch 15 Loss 6.9830\n",
            "Epoch 18 Loss 6.9830\n",
            "Time taken for 1 epoch: 3.477527379989624 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 6.9273\n",
            "Epoch 19 Batch 1 Loss 6.9320\n",
            "Epoch 19 Batch 2 Loss 6.9248\n",
            "Epoch 19 Batch 3 Loss 6.9213\n",
            "Epoch 19 Batch 4 Loss 6.9162\n",
            "Epoch 19 Batch 5 Loss 6.9223\n",
            "Epoch 19 Batch 6 Loss 6.9230\n",
            "Epoch 19 Batch 7 Loss 6.9207\n",
            "Epoch 19 Batch 8 Loss 6.9163\n",
            "Epoch 19 Batch 9 Loss 6.9187\n",
            "Epoch 19 Batch 10 Loss 6.9199\n",
            "Epoch 19 Batch 11 Loss 6.9248\n",
            "Epoch 19 Batch 12 Loss 6.9309\n",
            "Epoch 19 Batch 13 Loss 6.9253\n",
            "Epoch 19 Batch 14 Loss 6.9279\n",
            "Epoch 19 Batch 15 Loss 6.9243\n",
            "Epoch 19 Loss 6.9243\n",
            "Time taken for 1 epoch: 3.5051956176757812 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 6.9330\n",
            "Epoch 20 Batch 1 Loss 6.9008\n",
            "Epoch 20 Batch 2 Loss 6.8914\n",
            "Epoch 20 Batch 3 Loss 6.8762\n",
            "Epoch 20 Batch 4 Loss 6.8730\n",
            "Epoch 20 Batch 5 Loss 6.8675\n",
            "Epoch 20 Batch 6 Loss 6.8643\n",
            "Epoch 20 Batch 7 Loss 6.8545\n",
            "Epoch 20 Batch 8 Loss 6.8510\n",
            "Epoch 20 Batch 9 Loss 6.8484\n",
            "Epoch 20 Batch 10 Loss 6.8470\n",
            "Epoch 20 Batch 11 Loss 6.8504\n",
            "Epoch 20 Batch 12 Loss 6.8543\n",
            "Epoch 20 Batch 13 Loss 6.8563\n",
            "Epoch 20 Batch 14 Loss 6.8585\n",
            "Epoch 20 Batch 15 Loss 6.8602\n",
            "Epoch 20 Loss 6.8602\n",
            "Time taken for 1 epoch: 3.475377082824707 secs\n",
            "\n",
            "Epoch 21 Batch 0 Loss 6.7978\n",
            "Epoch 21 Batch 1 Loss 6.8468\n",
            "Epoch 21 Batch 2 Loss 6.8349\n",
            "Epoch 21 Batch 3 Loss 6.8110\n",
            "Epoch 21 Batch 4 Loss 6.8039\n",
            "Epoch 21 Batch 5 Loss 6.7936\n",
            "Epoch 21 Batch 6 Loss 6.7952\n",
            "Epoch 21 Batch 7 Loss 6.7978\n",
            "Epoch 21 Batch 8 Loss 6.7979\n",
            "Epoch 21 Batch 9 Loss 6.7928\n",
            "Epoch 21 Batch 10 Loss 6.7932\n",
            "Epoch 21 Batch 11 Loss 6.7852\n",
            "Epoch 21 Batch 12 Loss 6.7853\n",
            "Epoch 21 Batch 13 Loss 6.7809\n",
            "Epoch 21 Batch 14 Loss 6.7796\n",
            "Epoch 21 Batch 15 Loss 6.7759\n",
            "Epoch 21 Loss 6.7759\n",
            "Time taken for 1 epoch: 3.497310161590576 secs\n",
            "\n",
            "Epoch 22 Batch 0 Loss 6.7530\n",
            "Epoch 22 Batch 1 Loss 6.7226\n",
            "Epoch 22 Batch 2 Loss 6.7053\n",
            "Epoch 22 Batch 3 Loss 6.7053\n",
            "Epoch 22 Batch 4 Loss 6.7012\n",
            "Epoch 22 Batch 5 Loss 6.7072\n",
            "Epoch 22 Batch 6 Loss 6.7012\n",
            "Epoch 22 Batch 7 Loss 6.6992\n",
            "Epoch 22 Batch 8 Loss 6.6949\n",
            "Epoch 22 Batch 9 Loss 6.6928\n",
            "Epoch 22 Batch 10 Loss 6.6878\n",
            "Epoch 22 Batch 11 Loss 6.6829\n",
            "Epoch 22 Batch 12 Loss 6.6799\n",
            "Epoch 22 Batch 13 Loss 6.6841\n",
            "Epoch 22 Batch 14 Loss 6.6866\n",
            "Epoch 22 Batch 15 Loss 6.6855\n",
            "Epoch 22 Loss 6.6855\n",
            "Time taken for 1 epoch: 3.510730028152466 secs\n",
            "\n",
            "Epoch 23 Batch 0 Loss 6.5604\n",
            "Epoch 23 Batch 1 Loss 6.6448\n",
            "Epoch 23 Batch 2 Loss 6.6366\n",
            "Epoch 23 Batch 3 Loss 6.6203\n",
            "Epoch 23 Batch 4 Loss 6.6229\n",
            "Epoch 23 Batch 5 Loss 6.6165\n",
            "Epoch 23 Batch 6 Loss 6.6188\n",
            "Epoch 23 Batch 7 Loss 6.6105\n",
            "Epoch 23 Batch 8 Loss 6.6114\n",
            "Epoch 23 Batch 9 Loss 6.6071\n",
            "Epoch 23 Batch 10 Loss 6.6009\n",
            "Epoch 23 Batch 11 Loss 6.6004\n",
            "Epoch 23 Batch 12 Loss 6.6000\n",
            "Epoch 23 Batch 13 Loss 6.5973\n",
            "Epoch 23 Batch 14 Loss 6.5933\n",
            "Epoch 23 Batch 15 Loss 6.5948\n",
            "Epoch 23 Loss 6.5948\n",
            "Time taken for 1 epoch: 3.4902803897857666 secs\n",
            "\n",
            "Epoch 24 Batch 0 Loss 6.5477\n",
            "Epoch 24 Batch 1 Loss 6.4980\n",
            "Epoch 24 Batch 2 Loss 6.5006\n",
            "Epoch 24 Batch 3 Loss 6.4991\n",
            "Epoch 24 Batch 4 Loss 6.5170\n",
            "Epoch 24 Batch 5 Loss 6.5111\n",
            "Epoch 24 Batch 6 Loss 6.4988\n",
            "Epoch 24 Batch 7 Loss 6.4915\n",
            "Epoch 24 Batch 8 Loss 6.4987\n",
            "Epoch 24 Batch 9 Loss 6.4990\n",
            "Epoch 24 Batch 10 Loss 6.4951\n",
            "Epoch 24 Batch 11 Loss 6.4894\n",
            "Epoch 24 Batch 12 Loss 6.4849\n",
            "Epoch 24 Batch 13 Loss 6.4846\n",
            "Epoch 24 Batch 14 Loss 6.4852\n",
            "Epoch 24 Batch 15 Loss 6.4884\n",
            "Epoch 24 Loss 6.4884\n",
            "Time taken for 1 epoch: 3.513512372970581 secs\n",
            "\n",
            "Epoch 25 Batch 0 Loss 6.5085\n",
            "Epoch 25 Batch 1 Loss 6.4581\n",
            "Epoch 25 Batch 2 Loss 6.4029\n",
            "Epoch 25 Batch 3 Loss 6.4025\n",
            "Epoch 25 Batch 4 Loss 6.3901\n",
            "Epoch 25 Batch 5 Loss 6.3879\n",
            "Epoch 25 Batch 6 Loss 6.3885\n",
            "Epoch 25 Batch 7 Loss 6.3869\n",
            "Epoch 25 Batch 8 Loss 6.3902\n",
            "Epoch 25 Batch 9 Loss 6.3789\n",
            "Epoch 25 Batch 10 Loss 6.3836\n",
            "Epoch 25 Batch 11 Loss 6.3772\n",
            "Epoch 25 Batch 12 Loss 6.3774\n",
            "Epoch 25 Batch 13 Loss 6.3894\n",
            "Epoch 25 Batch 14 Loss 6.3952\n",
            "Epoch 25 Batch 15 Loss 6.3900\n",
            "Epoch 25 Loss 6.3900\n",
            "Time taken for 1 epoch: 3.516667127609253 secs\n",
            "\n",
            "Epoch 26 Batch 0 Loss 6.3969\n",
            "Epoch 26 Batch 1 Loss 6.3595\n",
            "Epoch 26 Batch 2 Loss 6.3605\n",
            "Epoch 26 Batch 3 Loss 6.3403\n",
            "Epoch 26 Batch 4 Loss 6.3324\n",
            "Epoch 26 Batch 5 Loss 6.3178\n",
            "Epoch 26 Batch 6 Loss 6.3162\n",
            "Epoch 26 Batch 7 Loss 6.3222\n",
            "Epoch 26 Batch 8 Loss 6.3117\n",
            "Epoch 26 Batch 9 Loss 6.3170\n",
            "Epoch 26 Batch 10 Loss 6.3158\n",
            "Epoch 26 Batch 11 Loss 6.3101\n",
            "Epoch 26 Batch 12 Loss 6.3055\n",
            "Epoch 26 Batch 13 Loss 6.2986\n",
            "Epoch 26 Batch 14 Loss 6.2956\n",
            "Epoch 26 Batch 15 Loss 6.2888\n",
            "Epoch 26 Loss 6.2888\n",
            "Time taken for 1 epoch: 3.5393149852752686 secs\n",
            "\n",
            "Epoch 27 Batch 0 Loss 6.2463\n",
            "Epoch 27 Batch 1 Loss 6.2029\n",
            "Epoch 27 Batch 2 Loss 6.1585\n",
            "Epoch 27 Batch 3 Loss 6.1819\n",
            "Epoch 27 Batch 4 Loss 6.1738\n",
            "Epoch 27 Batch 5 Loss 6.1756\n",
            "Epoch 27 Batch 6 Loss 6.1748\n",
            "Epoch 27 Batch 7 Loss 6.1882\n",
            "Epoch 27 Batch 8 Loss 6.1975\n",
            "Epoch 27 Batch 9 Loss 6.2094\n",
            "Epoch 27 Batch 10 Loss 6.2003\n",
            "Epoch 27 Batch 11 Loss 6.1943\n",
            "Epoch 27 Batch 12 Loss 6.1953\n",
            "Epoch 27 Batch 13 Loss 6.1910\n",
            "Epoch 27 Batch 14 Loss 6.1888\n",
            "Epoch 27 Batch 15 Loss 6.1900\n",
            "Epoch 27 Loss 6.1900\n",
            "Time taken for 1 epoch: 3.5377676486968994 secs\n",
            "\n",
            "Epoch 28 Batch 0 Loss 6.1581\n",
            "Epoch 28 Batch 1 Loss 6.1473\n",
            "Epoch 28 Batch 2 Loss 6.1471\n",
            "Epoch 28 Batch 3 Loss 6.1246\n",
            "Epoch 28 Batch 4 Loss 6.1297\n",
            "Epoch 28 Batch 5 Loss 6.1316\n",
            "Epoch 28 Batch 6 Loss 6.1369\n",
            "Epoch 28 Batch 7 Loss 6.1181\n",
            "Epoch 28 Batch 8 Loss 6.1150\n",
            "Epoch 28 Batch 9 Loss 6.1233\n",
            "Epoch 28 Batch 10 Loss 6.1218\n",
            "Epoch 28 Batch 11 Loss 6.1158\n",
            "Epoch 28 Batch 12 Loss 6.1160\n",
            "Epoch 28 Batch 13 Loss 6.1157\n",
            "Epoch 28 Batch 14 Loss 6.1105\n",
            "Epoch 28 Batch 15 Loss 6.1101\n",
            "Epoch 28 Loss 6.1101\n",
            "Time taken for 1 epoch: 3.527209997177124 secs\n",
            "\n",
            "Epoch 29 Batch 0 Loss 6.0211\n",
            "Epoch 29 Batch 1 Loss 6.0063\n",
            "Epoch 29 Batch 2 Loss 5.9965\n",
            "Epoch 29 Batch 3 Loss 5.9883\n",
            "Epoch 29 Batch 4 Loss 5.9854\n",
            "Epoch 29 Batch 5 Loss 5.9679\n",
            "Epoch 29 Batch 6 Loss 5.9708\n",
            "Epoch 29 Batch 7 Loss 5.9854\n",
            "Epoch 29 Batch 8 Loss 5.9919\n",
            "Epoch 29 Batch 9 Loss 6.0025\n",
            "Epoch 29 Batch 10 Loss 6.0117\n",
            "Epoch 29 Batch 11 Loss 6.0082\n",
            "Epoch 29 Batch 12 Loss 6.0132\n",
            "Epoch 29 Batch 13 Loss 6.0167\n",
            "Epoch 29 Batch 14 Loss 6.0188\n",
            "Epoch 29 Batch 15 Loss 6.0247\n",
            "Epoch 29 Loss 6.0247\n",
            "Time taken for 1 epoch: 3.528247594833374 secs\n",
            "\n",
            "Epoch 30 Batch 0 Loss 6.0317\n",
            "Epoch 30 Batch 1 Loss 5.9829\n",
            "Epoch 30 Batch 2 Loss 5.9666\n",
            "Epoch 30 Batch 3 Loss 5.9502\n",
            "Epoch 30 Batch 4 Loss 5.9538\n",
            "Epoch 30 Batch 5 Loss 5.9526\n",
            "Epoch 30 Batch 6 Loss 5.9446\n",
            "Epoch 30 Batch 7 Loss 5.9412\n",
            "Epoch 30 Batch 8 Loss 5.9311\n",
            "Epoch 30 Batch 9 Loss 5.9353\n",
            "Epoch 30 Batch 10 Loss 5.9271\n",
            "Epoch 30 Batch 11 Loss 5.9347\n",
            "Epoch 30 Batch 12 Loss 5.9317\n",
            "Epoch 30 Batch 13 Loss 5.9271\n",
            "Epoch 30 Batch 14 Loss 5.9248\n",
            "Epoch 30 Batch 15 Loss 5.9233\n",
            "Epoch 30 Loss 5.9233\n",
            "Time taken for 1 epoch: 3.5186731815338135 secs\n",
            "\n",
            "Epoch 31 Batch 0 Loss 5.9018\n",
            "Epoch 31 Batch 1 Loss 5.8835\n",
            "Epoch 31 Batch 2 Loss 5.8686\n",
            "Epoch 31 Batch 3 Loss 5.8636\n",
            "Epoch 31 Batch 4 Loss 5.8482\n",
            "Epoch 31 Batch 5 Loss 5.8294\n",
            "Epoch 31 Batch 6 Loss 5.8223\n",
            "Epoch 31 Batch 7 Loss 5.8146\n",
            "Epoch 31 Batch 8 Loss 5.8116\n",
            "Epoch 31 Batch 9 Loss 5.8122\n",
            "Epoch 31 Batch 10 Loss 5.8181\n",
            "Epoch 31 Batch 11 Loss 5.8116\n",
            "Epoch 31 Batch 12 Loss 5.8255\n",
            "Epoch 31 Batch 13 Loss 5.8279\n",
            "Epoch 31 Batch 14 Loss 5.8278\n",
            "Epoch 31 Batch 15 Loss 5.8318\n",
            "Epoch 31 Loss 5.8318\n",
            "Time taken for 1 epoch: 3.5144684314727783 secs\n",
            "\n",
            "Epoch 32 Batch 0 Loss 5.7506\n",
            "Epoch 32 Batch 1 Loss 5.7911\n",
            "Epoch 32 Batch 2 Loss 5.7578\n",
            "Epoch 32 Batch 3 Loss 5.7556\n",
            "Epoch 32 Batch 4 Loss 5.7483\n",
            "Epoch 32 Batch 5 Loss 5.7489\n",
            "Epoch 32 Batch 6 Loss 5.7316\n",
            "Epoch 32 Batch 7 Loss 5.7504\n",
            "Epoch 32 Batch 8 Loss 5.7440\n",
            "Epoch 32 Batch 9 Loss 5.7447\n",
            "Epoch 32 Batch 10 Loss 5.7480\n",
            "Epoch 32 Batch 11 Loss 5.7465\n",
            "Epoch 32 Batch 12 Loss 5.7431\n",
            "Epoch 32 Batch 13 Loss 5.7502\n",
            "Epoch 32 Batch 14 Loss 5.7514\n",
            "Epoch 32 Batch 15 Loss 5.7475\n",
            "Epoch 32 Loss 5.7475\n",
            "Time taken for 1 epoch: 3.541952610015869 secs\n",
            "\n",
            "Epoch 33 Batch 0 Loss 5.6010\n",
            "Epoch 33 Batch 1 Loss 5.6359\n",
            "Epoch 33 Batch 2 Loss 5.6683\n",
            "Epoch 33 Batch 3 Loss 5.6850\n",
            "Epoch 33 Batch 4 Loss 5.6800\n",
            "Epoch 33 Batch 5 Loss 5.6951\n",
            "Epoch 33 Batch 6 Loss 5.6975\n",
            "Epoch 33 Batch 7 Loss 5.6876\n",
            "Epoch 33 Batch 8 Loss 5.6733\n",
            "Epoch 33 Batch 9 Loss 5.6791\n",
            "Epoch 33 Batch 10 Loss 5.6740\n",
            "Epoch 33 Batch 11 Loss 5.6668\n",
            "Epoch 33 Batch 12 Loss 5.6703\n",
            "Epoch 33 Batch 13 Loss 5.6687\n",
            "Epoch 33 Batch 14 Loss 5.6674\n",
            "Epoch 33 Batch 15 Loss 5.6681\n",
            "Epoch 33 Loss 5.6681\n",
            "Time taken for 1 epoch: 3.516047477722168 secs\n",
            "\n",
            "Epoch 34 Batch 0 Loss 5.5720\n",
            "Epoch 34 Batch 1 Loss 5.5848\n",
            "Epoch 34 Batch 2 Loss 5.6150\n",
            "Epoch 34 Batch 3 Loss 5.6004\n",
            "Epoch 34 Batch 4 Loss 5.6181\n",
            "Epoch 34 Batch 5 Loss 5.6158\n",
            "Epoch 34 Batch 6 Loss 5.5883\n",
            "Epoch 34 Batch 7 Loss 5.5797\n",
            "Epoch 34 Batch 8 Loss 5.5757\n",
            "Epoch 34 Batch 9 Loss 5.5584\n",
            "Epoch 34 Batch 10 Loss 5.5634\n",
            "Epoch 34 Batch 11 Loss 5.5629\n",
            "Epoch 34 Batch 12 Loss 5.5650\n",
            "Epoch 34 Batch 13 Loss 5.5717\n",
            "Epoch 34 Batch 14 Loss 5.5706\n",
            "Epoch 34 Batch 15 Loss 5.5773\n",
            "Epoch 34 Loss 5.5773\n",
            "Time taken for 1 epoch: 3.5385916233062744 secs\n",
            "\n",
            "Epoch 35 Batch 0 Loss 5.6338\n",
            "Epoch 35 Batch 1 Loss 5.5929\n",
            "Epoch 35 Batch 2 Loss 5.5458\n",
            "Epoch 35 Batch 3 Loss 5.5432\n",
            "Epoch 35 Batch 4 Loss 5.5385\n",
            "Epoch 35 Batch 5 Loss 5.5396\n",
            "Epoch 35 Batch 6 Loss 5.5363\n",
            "Epoch 35 Batch 7 Loss 5.5231\n",
            "Epoch 35 Batch 8 Loss 5.5210\n",
            "Epoch 35 Batch 9 Loss 5.5075\n",
            "Epoch 35 Batch 10 Loss 5.5027\n",
            "Epoch 35 Batch 11 Loss 5.4972\n",
            "Epoch 35 Batch 12 Loss 5.4898\n",
            "Epoch 35 Batch 13 Loss 5.4897\n",
            "Epoch 35 Batch 14 Loss 5.4984\n",
            "Epoch 35 Batch 15 Loss 5.4963\n",
            "Epoch 35 Loss 5.4963\n",
            "Time taken for 1 epoch: 3.5457913875579834 secs\n",
            "\n",
            "Epoch 36 Batch 0 Loss 5.3316\n",
            "Epoch 36 Batch 1 Loss 5.4012\n",
            "Epoch 36 Batch 2 Loss 5.4221\n",
            "Epoch 36 Batch 3 Loss 5.4359\n",
            "Epoch 36 Batch 4 Loss 5.4292\n",
            "Epoch 36 Batch 5 Loss 5.4297\n",
            "Epoch 36 Batch 6 Loss 5.4244\n",
            "Epoch 36 Batch 7 Loss 5.4276\n",
            "Epoch 36 Batch 8 Loss 5.4253\n",
            "Epoch 36 Batch 9 Loss 5.4264\n",
            "Epoch 36 Batch 10 Loss 5.4234\n",
            "Epoch 36 Batch 11 Loss 5.4256\n",
            "Epoch 36 Batch 12 Loss 5.4323\n",
            "Epoch 36 Batch 13 Loss 5.4234\n",
            "Epoch 36 Batch 14 Loss 5.4240\n",
            "Epoch 36 Batch 15 Loss 5.4197\n",
            "Epoch 36 Loss 5.4197\n",
            "Time taken for 1 epoch: 3.5254197120666504 secs\n",
            "\n",
            "Epoch 37 Batch 0 Loss 5.3876\n",
            "Epoch 37 Batch 1 Loss 5.3934\n",
            "Epoch 37 Batch 2 Loss 5.3711\n",
            "Epoch 37 Batch 3 Loss 5.3525\n",
            "Epoch 37 Batch 4 Loss 5.3556\n",
            "Epoch 37 Batch 5 Loss 5.3594\n",
            "Epoch 37 Batch 6 Loss 5.3509\n",
            "Epoch 37 Batch 7 Loss 5.3483\n",
            "Epoch 37 Batch 8 Loss 5.3370\n",
            "Epoch 37 Batch 9 Loss 5.3409\n",
            "Epoch 37 Batch 10 Loss 5.3484\n",
            "Epoch 37 Batch 11 Loss 5.3482\n",
            "Epoch 37 Batch 12 Loss 5.3414\n",
            "Epoch 37 Batch 13 Loss 5.3331\n",
            "Epoch 37 Batch 14 Loss 5.3317\n",
            "Epoch 37 Batch 15 Loss 5.3277\n",
            "Epoch 37 Loss 5.3277\n",
            "Time taken for 1 epoch: 3.523965835571289 secs\n",
            "\n",
            "Epoch 38 Batch 0 Loss 5.2787\n",
            "Epoch 38 Batch 1 Loss 5.2914\n",
            "Epoch 38 Batch 2 Loss 5.2712\n",
            "Epoch 38 Batch 3 Loss 5.2446\n",
            "Epoch 38 Batch 4 Loss 5.2508\n",
            "Epoch 38 Batch 5 Loss 5.2542\n",
            "Epoch 38 Batch 6 Loss 5.2544\n",
            "Epoch 38 Batch 7 Loss 5.2531\n",
            "Epoch 38 Batch 8 Loss 5.2547\n",
            "Epoch 38 Batch 9 Loss 5.2584\n",
            "Epoch 38 Batch 10 Loss 5.2595\n",
            "Epoch 38 Batch 11 Loss 5.2572\n",
            "Epoch 38 Batch 12 Loss 5.2632\n",
            "Epoch 38 Batch 13 Loss 5.2612\n",
            "Epoch 38 Batch 14 Loss 5.2621\n",
            "Epoch 38 Batch 15 Loss 5.2486\n",
            "Epoch 38 Loss 5.2486\n",
            "Time taken for 1 epoch: 3.529871940612793 secs\n",
            "\n",
            "Epoch 39 Batch 0 Loss 5.3601\n",
            "Epoch 39 Batch 1 Loss 5.2563\n",
            "Epoch 39 Batch 2 Loss 5.2092\n",
            "Epoch 39 Batch 3 Loss 5.2086\n",
            "Epoch 39 Batch 4 Loss 5.2049\n",
            "Epoch 39 Batch 5 Loss 5.2163\n",
            "Epoch 39 Batch 6 Loss 5.2050\n",
            "Epoch 39 Batch 7 Loss 5.2018\n",
            "Epoch 39 Batch 8 Loss 5.2003\n",
            "Epoch 39 Batch 9 Loss 5.2042\n",
            "Epoch 39 Batch 10 Loss 5.2116\n",
            "Epoch 39 Batch 11 Loss 5.2121\n",
            "Epoch 39 Batch 12 Loss 5.1916\n",
            "Epoch 39 Batch 13 Loss 5.1809\n",
            "Epoch 39 Batch 14 Loss 5.1807\n",
            "Epoch 39 Batch 15 Loss 5.1795\n",
            "Epoch 39 Loss 5.1795\n",
            "Time taken for 1 epoch: 3.5264973640441895 secs\n",
            "\n",
            "Epoch 40 Batch 0 Loss 5.2129\n",
            "Epoch 40 Batch 1 Loss 5.1630\n",
            "Epoch 40 Batch 2 Loss 5.1789\n",
            "Epoch 40 Batch 3 Loss 5.1707\n",
            "Epoch 40 Batch 4 Loss 5.1361\n",
            "Epoch 40 Batch 5 Loss 5.1276\n",
            "Epoch 40 Batch 6 Loss 5.1312\n",
            "Epoch 40 Batch 7 Loss 5.1345\n",
            "Epoch 40 Batch 8 Loss 5.1270\n",
            "Epoch 40 Batch 9 Loss 5.1231\n",
            "Epoch 40 Batch 10 Loss 5.1197\n",
            "Epoch 40 Batch 11 Loss 5.1100\n",
            "Epoch 40 Batch 12 Loss 5.1069\n",
            "Epoch 40 Batch 13 Loss 5.0994\n",
            "Epoch 40 Batch 14 Loss 5.1004\n",
            "Epoch 40 Batch 15 Loss 5.0901\n",
            "Epoch 40 Loss 5.0901\n",
            "Time taken for 1 epoch: 3.5514771938323975 secs\n",
            "\n",
            "Epoch 41 Batch 0 Loss 5.0609\n",
            "Epoch 41 Batch 1 Loss 5.0492\n",
            "Epoch 41 Batch 2 Loss 5.0581\n",
            "Epoch 41 Batch 3 Loss 5.0442\n",
            "Epoch 41 Batch 4 Loss 5.0391\n",
            "Epoch 41 Batch 5 Loss 5.0266\n",
            "Epoch 41 Batch 6 Loss 5.0372\n",
            "Epoch 41 Batch 7 Loss 5.0278\n",
            "Epoch 41 Batch 8 Loss 5.0254\n",
            "Epoch 41 Batch 9 Loss 5.0197\n",
            "Epoch 41 Batch 10 Loss 5.0194\n",
            "Epoch 41 Batch 11 Loss 5.0161\n",
            "Epoch 41 Batch 12 Loss 5.0079\n",
            "Epoch 41 Batch 13 Loss 5.0016\n",
            "Epoch 41 Batch 14 Loss 5.0054\n",
            "Epoch 41 Batch 15 Loss 5.0029\n",
            "Epoch 41 Loss 5.0029\n",
            "Time taken for 1 epoch: 3.5440032482147217 secs\n",
            "\n",
            "Epoch 42 Batch 0 Loss 4.9713\n",
            "Epoch 42 Batch 1 Loss 4.9807\n",
            "Epoch 42 Batch 2 Loss 4.9560\n",
            "Epoch 42 Batch 3 Loss 4.9440\n",
            "Epoch 42 Batch 4 Loss 4.9304\n",
            "Epoch 42 Batch 5 Loss 4.9179\n",
            "Epoch 42 Batch 6 Loss 4.9060\n",
            "Epoch 42 Batch 7 Loss 4.9126\n",
            "Epoch 42 Batch 8 Loss 4.9131\n",
            "Epoch 42 Batch 9 Loss 4.9007\n",
            "Epoch 42 Batch 10 Loss 4.9087\n",
            "Epoch 42 Batch 11 Loss 4.9114\n",
            "Epoch 42 Batch 12 Loss 4.9182\n",
            "Epoch 42 Batch 13 Loss 4.9171\n",
            "Epoch 42 Batch 14 Loss 4.9177\n",
            "Epoch 42 Batch 15 Loss 4.9174\n",
            "Epoch 42 Loss 4.9174\n",
            "Time taken for 1 epoch: 3.542241096496582 secs\n",
            "\n",
            "Epoch 43 Batch 0 Loss 4.9711\n",
            "Epoch 43 Batch 1 Loss 4.9183\n",
            "Epoch 43 Batch 2 Loss 4.8817\n",
            "Epoch 43 Batch 3 Loss 4.8686\n",
            "Epoch 43 Batch 4 Loss 4.8508\n",
            "Epoch 43 Batch 5 Loss 4.8683\n",
            "Epoch 43 Batch 6 Loss 4.8577\n",
            "Epoch 43 Batch 7 Loss 4.8435\n",
            "Epoch 43 Batch 8 Loss 4.8342\n",
            "Epoch 43 Batch 9 Loss 4.8417\n",
            "Epoch 43 Batch 10 Loss 4.8477\n",
            "Epoch 43 Batch 11 Loss 4.8484\n",
            "Epoch 43 Batch 12 Loss 4.8440\n",
            "Epoch 43 Batch 13 Loss 4.8411\n",
            "Epoch 43 Batch 14 Loss 4.8408\n",
            "Epoch 43 Batch 15 Loss 4.8393\n",
            "Epoch 43 Loss 4.8393\n",
            "Time taken for 1 epoch: 3.5466902256011963 secs\n",
            "\n",
            "Epoch 44 Batch 0 Loss 4.7479\n",
            "Epoch 44 Batch 1 Loss 4.7714\n",
            "Epoch 44 Batch 2 Loss 4.7736\n",
            "Epoch 44 Batch 3 Loss 4.7924\n",
            "Epoch 44 Batch 4 Loss 4.7957\n",
            "Epoch 44 Batch 5 Loss 4.7814\n",
            "Epoch 44 Batch 6 Loss 4.7792\n",
            "Epoch 44 Batch 7 Loss 4.7744\n",
            "Epoch 44 Batch 8 Loss 4.7642\n",
            "Epoch 44 Batch 9 Loss 4.7741\n",
            "Epoch 44 Batch 10 Loss 4.7728\n",
            "Epoch 44 Batch 11 Loss 4.7643\n",
            "Epoch 44 Batch 12 Loss 4.7698\n",
            "Epoch 44 Batch 13 Loss 4.7710\n",
            "Epoch 44 Batch 14 Loss 4.7684\n",
            "Epoch 44 Batch 15 Loss 4.7569\n",
            "Epoch 44 Loss 4.7569\n",
            "Time taken for 1 epoch: 3.566908597946167 secs\n",
            "\n",
            "Epoch 45 Batch 0 Loss 4.6493\n",
            "Epoch 45 Batch 1 Loss 4.6432\n",
            "Epoch 45 Batch 2 Loss 4.7135\n",
            "Epoch 45 Batch 3 Loss 4.7074\n",
            "Epoch 45 Batch 4 Loss 4.7176\n",
            "Epoch 45 Batch 5 Loss 4.7176\n",
            "Epoch 45 Batch 6 Loss 4.7176\n",
            "Epoch 45 Batch 7 Loss 4.7134\n",
            "Epoch 45 Batch 8 Loss 4.7161\n",
            "Epoch 45 Batch 9 Loss 4.7129\n",
            "Epoch 45 Batch 10 Loss 4.7081\n",
            "Epoch 45 Batch 11 Loss 4.7018\n",
            "Epoch 45 Batch 12 Loss 4.6955\n",
            "Epoch 45 Batch 13 Loss 4.6956\n",
            "Epoch 45 Batch 14 Loss 4.6943\n",
            "Epoch 45 Batch 15 Loss 4.6903\n",
            "Epoch 45 Loss 4.6903\n",
            "Time taken for 1 epoch: 3.5451500415802 secs\n",
            "\n",
            "Epoch 46 Batch 0 Loss 4.6185\n",
            "Epoch 46 Batch 1 Loss 4.6963\n",
            "Epoch 46 Batch 2 Loss 4.6912\n",
            "Epoch 46 Batch 3 Loss 4.6559\n",
            "Epoch 46 Batch 4 Loss 4.6597\n",
            "Epoch 46 Batch 5 Loss 4.6494\n",
            "Epoch 46 Batch 6 Loss 4.6367\n",
            "Epoch 46 Batch 7 Loss 4.6245\n",
            "Epoch 46 Batch 8 Loss 4.6115\n",
            "Epoch 46 Batch 9 Loss 4.6129\n",
            "Epoch 46 Batch 10 Loss 4.6141\n",
            "Epoch 46 Batch 11 Loss 4.6008\n",
            "Epoch 46 Batch 12 Loss 4.5981\n",
            "Epoch 46 Batch 13 Loss 4.6003\n",
            "Epoch 46 Batch 14 Loss 4.5965\n",
            "Epoch 46 Batch 15 Loss 4.5888\n",
            "Epoch 46 Loss 4.5888\n",
            "Time taken for 1 epoch: 3.564443349838257 secs\n",
            "\n",
            "Epoch 47 Batch 0 Loss 4.6184\n",
            "Epoch 47 Batch 1 Loss 4.5878\n",
            "Epoch 47 Batch 2 Loss 4.5663\n",
            "Epoch 47 Batch 3 Loss 4.5245\n",
            "Epoch 47 Batch 4 Loss 4.5303\n",
            "Epoch 47 Batch 5 Loss 4.5040\n",
            "Epoch 47 Batch 6 Loss 4.5077\n",
            "Epoch 47 Batch 7 Loss 4.5104\n",
            "Epoch 47 Batch 8 Loss 4.5094\n",
            "Epoch 47 Batch 9 Loss 4.5010\n",
            "Epoch 47 Batch 10 Loss 4.5072\n",
            "Epoch 47 Batch 11 Loss 4.4993\n",
            "Epoch 47 Batch 12 Loss 4.4937\n",
            "Epoch 47 Batch 13 Loss 4.5031\n",
            "Epoch 47 Batch 14 Loss 4.4987\n",
            "Epoch 47 Batch 15 Loss 4.5042\n",
            "Epoch 47 Loss 4.5042\n",
            "Time taken for 1 epoch: 3.574129819869995 secs\n",
            "\n",
            "Epoch 48 Batch 0 Loss 4.4668\n",
            "Epoch 48 Batch 1 Loss 4.4129\n",
            "Epoch 48 Batch 2 Loss 4.4269\n",
            "Epoch 48 Batch 3 Loss 4.3922\n",
            "Epoch 48 Batch 4 Loss 4.3760\n",
            "Epoch 48 Batch 5 Loss 4.3842\n",
            "Epoch 48 Batch 6 Loss 4.3892\n",
            "Epoch 48 Batch 7 Loss 4.3815\n",
            "Epoch 48 Batch 8 Loss 4.3762\n",
            "Epoch 48 Batch 9 Loss 4.3826\n",
            "Epoch 48 Batch 10 Loss 4.3925\n",
            "Epoch 48 Batch 11 Loss 4.3993\n",
            "Epoch 48 Batch 12 Loss 4.4095\n",
            "Epoch 48 Batch 13 Loss 4.4089\n",
            "Epoch 48 Batch 14 Loss 4.4070\n",
            "Epoch 48 Batch 15 Loss 4.4092\n",
            "Epoch 48 Loss 4.4092\n",
            "Time taken for 1 epoch: 3.5604050159454346 secs\n",
            "\n",
            "Epoch 49 Batch 0 Loss 4.2948\n",
            "Epoch 49 Batch 1 Loss 4.2403\n",
            "Epoch 49 Batch 2 Loss 4.2934\n",
            "Epoch 49 Batch 3 Loss 4.3125\n",
            "Epoch 49 Batch 4 Loss 4.3169\n",
            "Epoch 49 Batch 5 Loss 4.3041\n",
            "Epoch 49 Batch 6 Loss 4.3091\n",
            "Epoch 49 Batch 7 Loss 4.3263\n",
            "Epoch 49 Batch 8 Loss 4.3247\n",
            "Epoch 49 Batch 9 Loss 4.3194\n",
            "Epoch 49 Batch 10 Loss 4.3212\n",
            "Epoch 49 Batch 11 Loss 4.3244\n",
            "Epoch 49 Batch 12 Loss 4.3304\n",
            "Epoch 49 Batch 13 Loss 4.3358\n",
            "Epoch 49 Batch 14 Loss 4.3284\n",
            "Epoch 49 Batch 15 Loss 4.3204\n",
            "Epoch 49 Loss 4.3204\n",
            "Time taken for 1 epoch: 3.5574557781219482 secs\n",
            "\n",
            "Epoch 50 Batch 0 Loss 4.1481\n",
            "Epoch 50 Batch 1 Loss 4.2352\n",
            "Epoch 50 Batch 2 Loss 4.2450\n",
            "Epoch 50 Batch 3 Loss 4.2160\n",
            "Epoch 50 Batch 4 Loss 4.2228\n",
            "Epoch 50 Batch 5 Loss 4.2102\n",
            "Epoch 50 Batch 6 Loss 4.1791\n",
            "Epoch 50 Batch 7 Loss 4.2096\n",
            "Epoch 50 Batch 8 Loss 4.2254\n",
            "Epoch 50 Batch 9 Loss 4.2257\n",
            "Epoch 50 Batch 10 Loss 4.2265\n",
            "Epoch 50 Batch 11 Loss 4.2190\n",
            "Epoch 50 Batch 12 Loss 4.2172\n",
            "Epoch 50 Batch 13 Loss 4.2140\n",
            "Epoch 50 Batch 14 Loss 4.2210\n",
            "Epoch 50 Batch 15 Loss 4.2214\n",
            "Epoch 50 Loss 4.2214\n",
            "Time taken for 1 epoch: 3.5713794231414795 secs\n",
            "\n",
            "Epoch 51 Batch 0 Loss 4.1454\n",
            "Epoch 51 Batch 1 Loss 4.1836\n",
            "Epoch 51 Batch 2 Loss 4.1225\n",
            "Epoch 51 Batch 3 Loss 4.1292\n",
            "Epoch 51 Batch 4 Loss 4.1325\n",
            "Epoch 51 Batch 5 Loss 4.1339\n",
            "Epoch 51 Batch 6 Loss 4.1263\n",
            "Epoch 51 Batch 7 Loss 4.1190\n",
            "Epoch 51 Batch 8 Loss 4.1285\n",
            "Epoch 51 Batch 9 Loss 4.1198\n",
            "Epoch 51 Batch 10 Loss 4.1164\n",
            "Epoch 51 Batch 11 Loss 4.1149\n",
            "Epoch 51 Batch 12 Loss 4.1215\n",
            "Epoch 51 Batch 13 Loss 4.1231\n",
            "Epoch 51 Batch 14 Loss 4.1300\n",
            "Epoch 51 Batch 15 Loss 4.1308\n",
            "Epoch 51 Loss 4.1308\n",
            "Time taken for 1 epoch: 3.5659358501434326 secs\n",
            "\n",
            "Epoch 52 Batch 0 Loss 4.1496\n",
            "Epoch 52 Batch 1 Loss 4.1016\n",
            "Epoch 52 Batch 2 Loss 4.1173\n",
            "Epoch 52 Batch 3 Loss 4.0871\n",
            "Epoch 52 Batch 4 Loss 4.1064\n",
            "Epoch 52 Batch 5 Loss 4.0818\n",
            "Epoch 52 Batch 6 Loss 4.0880\n",
            "Epoch 52 Batch 7 Loss 4.0789\n",
            "Epoch 52 Batch 8 Loss 4.0517\n",
            "Epoch 52 Batch 9 Loss 4.0484\n",
            "Epoch 52 Batch 10 Loss 4.0430\n",
            "Epoch 52 Batch 11 Loss 4.0382\n",
            "Epoch 52 Batch 12 Loss 4.0318\n",
            "Epoch 52 Batch 13 Loss 4.0340\n",
            "Epoch 52 Batch 14 Loss 4.0344\n",
            "Epoch 52 Batch 15 Loss 4.0317\n",
            "Epoch 52 Loss 4.0317\n",
            "Time taken for 1 epoch: 3.546682119369507 secs\n",
            "\n",
            "Epoch 53 Batch 0 Loss 3.9442\n",
            "Epoch 53 Batch 1 Loss 3.9568\n",
            "Epoch 53 Batch 2 Loss 3.9559\n",
            "Epoch 53 Batch 3 Loss 3.9750\n",
            "Epoch 53 Batch 4 Loss 3.9522\n",
            "Epoch 53 Batch 5 Loss 3.9522\n",
            "Epoch 53 Batch 6 Loss 3.9448\n",
            "Epoch 53 Batch 7 Loss 3.9533\n",
            "Epoch 53 Batch 8 Loss 3.9323\n",
            "Epoch 53 Batch 9 Loss 3.9263\n",
            "Epoch 53 Batch 10 Loss 3.9263\n",
            "Epoch 53 Batch 11 Loss 3.9303\n",
            "Epoch 53 Batch 12 Loss 3.9241\n",
            "Epoch 53 Batch 13 Loss 3.9285\n",
            "Epoch 53 Batch 14 Loss 3.9302\n",
            "Epoch 53 Batch 15 Loss 3.9347\n",
            "Epoch 53 Loss 3.9347\n",
            "Time taken for 1 epoch: 3.568128824234009 secs\n",
            "\n",
            "Epoch 54 Batch 0 Loss 3.8972\n",
            "Epoch 54 Batch 1 Loss 3.8640\n",
            "Epoch 54 Batch 2 Loss 3.8532\n",
            "Epoch 54 Batch 3 Loss 3.8542\n",
            "Epoch 54 Batch 4 Loss 3.8491\n",
            "Epoch 54 Batch 5 Loss 3.8474\n",
            "Epoch 54 Batch 6 Loss 3.8605\n",
            "Epoch 54 Batch 7 Loss 3.8609\n",
            "Epoch 54 Batch 8 Loss 3.8556\n",
            "Epoch 54 Batch 9 Loss 3.8499\n",
            "Epoch 54 Batch 10 Loss 3.8484\n",
            "Epoch 54 Batch 11 Loss 3.8500\n",
            "Epoch 54 Batch 12 Loss 3.8521\n",
            "Epoch 54 Batch 13 Loss 3.8525\n",
            "Epoch 54 Batch 14 Loss 3.8439\n",
            "Epoch 54 Batch 15 Loss 3.8487\n",
            "Epoch 54 Loss 3.8487\n",
            "Time taken for 1 epoch: 3.575252056121826 secs\n",
            "\n",
            "Epoch 55 Batch 0 Loss 3.7383\n",
            "Epoch 55 Batch 1 Loss 3.7761\n",
            "Epoch 55 Batch 2 Loss 3.7634\n",
            "Epoch 55 Batch 3 Loss 3.7423\n",
            "Epoch 55 Batch 4 Loss 3.7403\n",
            "Epoch 55 Batch 5 Loss 3.7536\n",
            "Epoch 55 Batch 6 Loss 3.7498\n",
            "Epoch 55 Batch 7 Loss 3.7570\n",
            "Epoch 55 Batch 8 Loss 3.7561\n",
            "Epoch 55 Batch 9 Loss 3.7585\n",
            "Epoch 55 Batch 10 Loss 3.7557\n",
            "Epoch 55 Batch 11 Loss 3.7591\n",
            "Epoch 55 Batch 12 Loss 3.7603\n",
            "Epoch 55 Batch 13 Loss 3.7606\n",
            "Epoch 55 Batch 14 Loss 3.7466\n",
            "Epoch 55 Batch 15 Loss 3.7440\n",
            "Epoch 55 Loss 3.7440\n",
            "Time taken for 1 epoch: 3.546168565750122 secs\n",
            "\n",
            "Epoch 56 Batch 0 Loss 3.6287\n",
            "Epoch 56 Batch 1 Loss 3.6688\n",
            "Epoch 56 Batch 2 Loss 3.6630\n",
            "Epoch 56 Batch 3 Loss 3.6479\n",
            "Epoch 56 Batch 4 Loss 3.6427\n",
            "Epoch 56 Batch 5 Loss 3.6404\n",
            "Epoch 56 Batch 6 Loss 3.6420\n",
            "Epoch 56 Batch 7 Loss 3.6321\n",
            "Epoch 56 Batch 8 Loss 3.6232\n",
            "Epoch 56 Batch 9 Loss 3.6220\n",
            "Epoch 56 Batch 10 Loss 3.6271\n",
            "Epoch 56 Batch 11 Loss 3.6278\n",
            "Epoch 56 Batch 12 Loss 3.6258\n",
            "Epoch 56 Batch 13 Loss 3.6323\n",
            "Epoch 56 Batch 14 Loss 3.6312\n",
            "Epoch 56 Batch 15 Loss 3.6383\n",
            "Epoch 56 Loss 3.6383\n",
            "Time taken for 1 epoch: 3.568988084793091 secs\n",
            "\n",
            "Epoch 57 Batch 0 Loss 3.5400\n",
            "Epoch 57 Batch 1 Loss 3.5544\n",
            "Epoch 57 Batch 2 Loss 3.5966\n",
            "Epoch 57 Batch 3 Loss 3.5675\n",
            "Epoch 57 Batch 4 Loss 3.5712\n",
            "Epoch 57 Batch 5 Loss 3.5889\n",
            "Epoch 57 Batch 6 Loss 3.5762\n",
            "Epoch 57 Batch 7 Loss 3.5721\n",
            "Epoch 57 Batch 8 Loss 3.5612\n",
            "Epoch 57 Batch 9 Loss 3.5526\n",
            "Epoch 57 Batch 10 Loss 3.5547\n",
            "Epoch 57 Batch 11 Loss 3.5505\n",
            "Epoch 57 Batch 12 Loss 3.5468\n",
            "Epoch 57 Batch 13 Loss 3.5456\n",
            "Epoch 57 Batch 14 Loss 3.5418\n",
            "Epoch 57 Batch 15 Loss 3.5353\n",
            "Epoch 57 Loss 3.5353\n",
            "Time taken for 1 epoch: 3.5552425384521484 secs\n",
            "\n",
            "Epoch 58 Batch 0 Loss 3.3942\n",
            "Epoch 58 Batch 1 Loss 3.4457\n",
            "Epoch 58 Batch 2 Loss 3.4463\n",
            "Epoch 58 Batch 3 Loss 3.4569\n",
            "Epoch 58 Batch 4 Loss 3.4547\n",
            "Epoch 58 Batch 5 Loss 3.4662\n",
            "Epoch 58 Batch 6 Loss 3.4638\n",
            "Epoch 58 Batch 7 Loss 3.4621\n",
            "Epoch 58 Batch 8 Loss 3.4562\n",
            "Epoch 58 Batch 9 Loss 3.4505\n",
            "Epoch 58 Batch 10 Loss 3.4516\n",
            "Epoch 58 Batch 11 Loss 3.4430\n",
            "Epoch 58 Batch 12 Loss 3.4325\n",
            "Epoch 58 Batch 13 Loss 3.4326\n",
            "Epoch 58 Batch 14 Loss 3.4374\n",
            "Epoch 58 Batch 15 Loss 3.4421\n",
            "Epoch 58 Loss 3.4421\n",
            "Time taken for 1 epoch: 3.5727438926696777 secs\n",
            "\n",
            "Epoch 59 Batch 0 Loss 3.3268\n",
            "Epoch 59 Batch 1 Loss 3.3589\n",
            "Epoch 59 Batch 2 Loss 3.3659\n",
            "Epoch 59 Batch 3 Loss 3.3812\n",
            "Epoch 59 Batch 4 Loss 3.3808\n",
            "Epoch 59 Batch 5 Loss 3.3784\n",
            "Epoch 59 Batch 6 Loss 3.3956\n",
            "Epoch 59 Batch 7 Loss 3.3791\n",
            "Epoch 59 Batch 8 Loss 3.3797\n",
            "Epoch 59 Batch 9 Loss 3.3691\n",
            "Epoch 59 Batch 10 Loss 3.3597\n",
            "Epoch 59 Batch 11 Loss 3.3498\n",
            "Epoch 59 Batch 12 Loss 3.3399\n",
            "Epoch 59 Batch 13 Loss 3.3380\n",
            "Epoch 59 Batch 14 Loss 3.3366\n",
            "Epoch 59 Batch 15 Loss 3.3330\n",
            "Epoch 59 Loss 3.3330\n",
            "Time taken for 1 epoch: 3.575137138366699 secs\n",
            "\n",
            "Epoch 60 Batch 0 Loss 3.2336\n",
            "Epoch 60 Batch 1 Loss 3.2303\n",
            "Epoch 60 Batch 2 Loss 3.2403\n",
            "Epoch 60 Batch 3 Loss 3.2601\n",
            "Epoch 60 Batch 4 Loss 3.2705\n",
            "Epoch 60 Batch 5 Loss 3.2662\n",
            "Epoch 60 Batch 6 Loss 3.2532\n",
            "Epoch 60 Batch 7 Loss 3.2522\n",
            "Epoch 60 Batch 8 Loss 3.2446\n",
            "Epoch 60 Batch 9 Loss 3.2482\n",
            "Epoch 60 Batch 10 Loss 3.2353\n",
            "Epoch 60 Batch 11 Loss 3.2357\n",
            "Epoch 60 Batch 12 Loss 3.2248\n",
            "Epoch 60 Batch 13 Loss 3.2318\n",
            "Epoch 60 Batch 14 Loss 3.2343\n",
            "Epoch 60 Batch 15 Loss 3.2356\n",
            "Epoch 60 Loss 3.2356\n",
            "Time taken for 1 epoch: 3.5686991214752197 secs\n",
            "\n",
            "Epoch 61 Batch 0 Loss 3.0296\n",
            "Epoch 61 Batch 1 Loss 3.0857\n",
            "Epoch 61 Batch 2 Loss 3.1369\n",
            "Epoch 61 Batch 3 Loss 3.1128\n",
            "Epoch 61 Batch 4 Loss 3.0940\n",
            "Epoch 61 Batch 5 Loss 3.1032\n",
            "Epoch 61 Batch 6 Loss 3.1277\n",
            "Epoch 61 Batch 7 Loss 3.1262\n",
            "Epoch 61 Batch 8 Loss 3.1204\n",
            "Epoch 61 Batch 9 Loss 3.1351\n",
            "Epoch 61 Batch 10 Loss 3.1246\n",
            "Epoch 61 Batch 11 Loss 3.1261\n",
            "Epoch 61 Batch 12 Loss 3.1254\n",
            "Epoch 61 Batch 13 Loss 3.1282\n",
            "Epoch 61 Batch 14 Loss 3.1256\n",
            "Epoch 61 Batch 15 Loss 3.1271\n",
            "Epoch 61 Loss 3.1271\n",
            "Time taken for 1 epoch: 3.582103967666626 secs\n",
            "\n",
            "Epoch 62 Batch 0 Loss 3.1845\n",
            "Epoch 62 Batch 1 Loss 3.1157\n",
            "Epoch 62 Batch 2 Loss 3.0723\n",
            "Epoch 62 Batch 3 Loss 3.0655\n",
            "Epoch 62 Batch 4 Loss 3.0387\n",
            "Epoch 62 Batch 5 Loss 3.0539\n",
            "Epoch 62 Batch 6 Loss 3.0587\n",
            "Epoch 62 Batch 7 Loss 3.0648\n",
            "Epoch 62 Batch 8 Loss 3.0571\n",
            "Epoch 62 Batch 9 Loss 3.0568\n",
            "Epoch 62 Batch 10 Loss 3.0563\n",
            "Epoch 62 Batch 11 Loss 3.0453\n",
            "Epoch 62 Batch 12 Loss 3.0361\n",
            "Epoch 62 Batch 13 Loss 3.0278\n",
            "Epoch 62 Batch 14 Loss 3.0264\n",
            "Epoch 62 Batch 15 Loss 3.0270\n",
            "Epoch 62 Loss 3.0270\n",
            "Time taken for 1 epoch: 3.567680597305298 secs\n",
            "\n",
            "Epoch 63 Batch 0 Loss 2.9563\n",
            "Epoch 63 Batch 1 Loss 2.9432\n",
            "Epoch 63 Batch 2 Loss 2.9689\n",
            "Epoch 63 Batch 3 Loss 2.9486\n",
            "Epoch 63 Batch 4 Loss 2.9524\n",
            "Epoch 63 Batch 5 Loss 2.9398\n",
            "Epoch 63 Batch 6 Loss 2.9334\n",
            "Epoch 63 Batch 7 Loss 2.9381\n",
            "Epoch 63 Batch 8 Loss 2.9383\n",
            "Epoch 63 Batch 9 Loss 2.9360\n",
            "Epoch 63 Batch 10 Loss 2.9370\n",
            "Epoch 63 Batch 11 Loss 2.9369\n",
            "Epoch 63 Batch 12 Loss 2.9259\n",
            "Epoch 63 Batch 13 Loss 2.9237\n",
            "Epoch 63 Batch 14 Loss 2.9202\n",
            "Epoch 63 Batch 15 Loss 2.9085\n",
            "Epoch 63 Loss 2.9085\n",
            "Time taken for 1 epoch: 3.590195417404175 secs\n",
            "\n",
            "Epoch 64 Batch 0 Loss 2.9105\n",
            "Epoch 64 Batch 1 Loss 2.8931\n",
            "Epoch 64 Batch 2 Loss 2.8728\n",
            "Epoch 64 Batch 3 Loss 2.8660\n",
            "Epoch 64 Batch 4 Loss 2.8510\n",
            "Epoch 64 Batch 5 Loss 2.8428\n",
            "Epoch 64 Batch 6 Loss 2.8500\n",
            "Epoch 64 Batch 7 Loss 2.8397\n",
            "Epoch 64 Batch 8 Loss 2.8373\n",
            "Epoch 64 Batch 9 Loss 2.8474\n",
            "Epoch 64 Batch 10 Loss 2.8488\n",
            "Epoch 64 Batch 11 Loss 2.8377\n",
            "Epoch 64 Batch 12 Loss 2.8320\n",
            "Epoch 64 Batch 13 Loss 2.8293\n",
            "Epoch 64 Batch 14 Loss 2.8233\n",
            "Epoch 64 Batch 15 Loss 2.8189\n",
            "Epoch 64 Loss 2.8189\n",
            "Time taken for 1 epoch: 3.587023973464966 secs\n",
            "\n",
            "Epoch 65 Batch 0 Loss 2.7149\n",
            "Epoch 65 Batch 1 Loss 2.7562\n",
            "Epoch 65 Batch 2 Loss 2.7498\n",
            "Epoch 65 Batch 3 Loss 2.7748\n",
            "Epoch 65 Batch 4 Loss 2.7589\n",
            "Epoch 65 Batch 5 Loss 2.7280\n",
            "Epoch 65 Batch 6 Loss 2.7328\n",
            "Epoch 65 Batch 7 Loss 2.7294\n",
            "Epoch 65 Batch 8 Loss 2.7278\n",
            "Epoch 65 Batch 9 Loss 2.7303\n",
            "Epoch 65 Batch 10 Loss 2.7057\n",
            "Epoch 65 Batch 11 Loss 2.7069\n",
            "Epoch 65 Batch 12 Loss 2.7187\n",
            "Epoch 65 Batch 13 Loss 2.7191\n",
            "Epoch 65 Batch 14 Loss 2.7130\n",
            "Epoch 65 Batch 15 Loss 2.7118\n",
            "Epoch 65 Loss 2.7118\n",
            "Time taken for 1 epoch: 3.5730090141296387 secs\n",
            "\n",
            "Epoch 66 Batch 0 Loss 2.5835\n",
            "Epoch 66 Batch 1 Loss 2.6117\n",
            "Epoch 66 Batch 2 Loss 2.5953\n",
            "Epoch 66 Batch 3 Loss 2.5929\n",
            "Epoch 66 Batch 4 Loss 2.6152\n",
            "Epoch 66 Batch 5 Loss 2.6140\n",
            "Epoch 66 Batch 6 Loss 2.6304\n",
            "Epoch 66 Batch 7 Loss 2.6425\n",
            "Epoch 66 Batch 8 Loss 2.6431\n",
            "Epoch 66 Batch 9 Loss 2.6424\n",
            "Epoch 66 Batch 10 Loss 2.6365\n",
            "Epoch 66 Batch 11 Loss 2.6397\n",
            "Epoch 66 Batch 12 Loss 2.6135\n",
            "Epoch 66 Batch 13 Loss 2.5993\n",
            "Epoch 66 Batch 14 Loss 2.6033\n",
            "Epoch 66 Batch 15 Loss 2.6053\n",
            "Epoch 66 Loss 2.6053\n",
            "Time taken for 1 epoch: 3.566896915435791 secs\n",
            "\n",
            "Epoch 67 Batch 0 Loss 2.4285\n",
            "Epoch 67 Batch 1 Loss 2.4980\n",
            "Epoch 67 Batch 2 Loss 2.5323\n",
            "Epoch 67 Batch 3 Loss 2.5552\n",
            "Epoch 67 Batch 4 Loss 2.5216\n",
            "Epoch 67 Batch 5 Loss 2.5273\n",
            "Epoch 67 Batch 6 Loss 2.5273\n",
            "Epoch 67 Batch 7 Loss 2.4874\n",
            "Epoch 67 Batch 8 Loss 2.4912\n",
            "Epoch 67 Batch 9 Loss 2.4888\n",
            "Epoch 67 Batch 10 Loss 2.4845\n",
            "Epoch 67 Batch 11 Loss 2.4724\n",
            "Epoch 67 Batch 12 Loss 2.4758\n",
            "Epoch 67 Batch 13 Loss 2.4868\n",
            "Epoch 67 Batch 14 Loss 2.4852\n",
            "Epoch 67 Batch 15 Loss 2.4877\n",
            "Epoch 67 Loss 2.4877\n",
            "Time taken for 1 epoch: 3.6025869846343994 secs\n",
            "\n",
            "Epoch 68 Batch 0 Loss 2.4209\n",
            "Epoch 68 Batch 1 Loss 2.4037\n",
            "Epoch 68 Batch 2 Loss 2.4143\n",
            "Epoch 68 Batch 3 Loss 2.4003\n",
            "Epoch 68 Batch 4 Loss 2.4045\n",
            "Epoch 68 Batch 5 Loss 2.4113\n",
            "Epoch 68 Batch 6 Loss 2.3894\n",
            "Epoch 68 Batch 7 Loss 2.3717\n",
            "Epoch 68 Batch 8 Loss 2.3671\n",
            "Epoch 68 Batch 9 Loss 2.3671\n",
            "Epoch 68 Batch 10 Loss 2.3656\n",
            "Epoch 68 Batch 11 Loss 2.3642\n",
            "Epoch 68 Batch 12 Loss 2.3710\n",
            "Epoch 68 Batch 13 Loss 2.3731\n",
            "Epoch 68 Batch 14 Loss 2.3741\n",
            "Epoch 68 Batch 15 Loss 2.3819\n",
            "Epoch 68 Loss 2.3819\n",
            "Time taken for 1 epoch: 3.600604295730591 secs\n",
            "\n",
            "Epoch 69 Batch 0 Loss 2.2474\n",
            "Epoch 69 Batch 1 Loss 2.3336\n",
            "Epoch 69 Batch 2 Loss 2.3297\n",
            "Epoch 69 Batch 3 Loss 2.2965\n",
            "Epoch 69 Batch 4 Loss 2.3020\n",
            "Epoch 69 Batch 5 Loss 2.3030\n",
            "Epoch 69 Batch 6 Loss 2.2924\n",
            "Epoch 69 Batch 7 Loss 2.2897\n",
            "Epoch 69 Batch 8 Loss 2.2753\n",
            "Epoch 69 Batch 9 Loss 2.2763\n",
            "Epoch 69 Batch 10 Loss 2.2797\n",
            "Epoch 69 Batch 11 Loss 2.2772\n",
            "Epoch 69 Batch 12 Loss 2.2753\n",
            "Epoch 69 Batch 13 Loss 2.2743\n",
            "Epoch 69 Batch 14 Loss 2.2793\n",
            "Epoch 69 Batch 15 Loss 2.2834\n",
            "Epoch 69 Loss 2.2834\n",
            "Time taken for 1 epoch: 3.570148468017578 secs\n",
            "\n",
            "Epoch 70 Batch 0 Loss 2.1570\n",
            "Epoch 70 Batch 1 Loss 2.1571\n",
            "Epoch 70 Batch 2 Loss 2.1802\n",
            "Epoch 70 Batch 3 Loss 2.1862\n",
            "Epoch 70 Batch 4 Loss 2.1984\n",
            "Epoch 70 Batch 5 Loss 2.1887\n",
            "Epoch 70 Batch 6 Loss 2.1769\n",
            "Epoch 70 Batch 7 Loss 2.1890\n",
            "Epoch 70 Batch 8 Loss 2.1938\n",
            "Epoch 70 Batch 9 Loss 2.1902\n",
            "Epoch 70 Batch 10 Loss 2.1743\n",
            "Epoch 70 Batch 11 Loss 2.1698\n",
            "Epoch 70 Batch 12 Loss 2.1599\n",
            "Epoch 70 Batch 13 Loss 2.1593\n",
            "Epoch 70 Batch 14 Loss 2.1679\n",
            "Epoch 70 Batch 15 Loss 2.1697\n",
            "Epoch 70 Loss 2.1697\n",
            "Time taken for 1 epoch: 3.575939416885376 secs\n",
            "\n",
            "Epoch 71 Batch 0 Loss 2.0982\n",
            "Epoch 71 Batch 1 Loss 2.0950\n",
            "Epoch 71 Batch 2 Loss 2.0856\n",
            "Epoch 71 Batch 3 Loss 2.0589\n",
            "Epoch 71 Batch 4 Loss 2.0649\n",
            "Epoch 71 Batch 5 Loss 2.0768\n",
            "Epoch 71 Batch 6 Loss 2.0828\n",
            "Epoch 71 Batch 7 Loss 2.0860\n",
            "Epoch 71 Batch 8 Loss 2.0955\n",
            "Epoch 71 Batch 9 Loss 2.0998\n",
            "Epoch 71 Batch 10 Loss 2.1080\n",
            "Epoch 71 Batch 11 Loss 2.1127\n",
            "Epoch 71 Batch 12 Loss 2.0994\n",
            "Epoch 71 Batch 13 Loss 2.0826\n",
            "Epoch 71 Batch 14 Loss 2.0770\n",
            "Epoch 71 Batch 15 Loss 2.0722\n",
            "Epoch 71 Loss 2.0722\n",
            "Time taken for 1 epoch: 3.578509569168091 secs\n",
            "\n",
            "Epoch 72 Batch 0 Loss 2.0293\n",
            "Epoch 72 Batch 1 Loss 2.0946\n",
            "Epoch 72 Batch 2 Loss 2.0244\n",
            "Epoch 72 Batch 3 Loss 2.0151\n",
            "Epoch 72 Batch 4 Loss 1.9991\n",
            "Epoch 72 Batch 5 Loss 2.0032\n",
            "Epoch 72 Batch 6 Loss 1.9996\n",
            "Epoch 72 Batch 7 Loss 1.9909\n",
            "Epoch 72 Batch 8 Loss 1.9959\n",
            "Epoch 72 Batch 9 Loss 2.0035\n",
            "Epoch 72 Batch 10 Loss 1.9958\n",
            "Epoch 72 Batch 11 Loss 1.9860\n",
            "Epoch 72 Batch 12 Loss 1.9790\n",
            "Epoch 72 Batch 13 Loss 1.9770\n",
            "Epoch 72 Batch 14 Loss 1.9795\n",
            "Epoch 72 Batch 15 Loss 1.9683\n",
            "Epoch 72 Loss 1.9683\n",
            "Time taken for 1 epoch: 3.5934548377990723 secs\n",
            "\n",
            "Epoch 73 Batch 0 Loss 1.8752\n",
            "Epoch 73 Batch 1 Loss 1.8151\n",
            "Epoch 73 Batch 2 Loss 1.8552\n",
            "Epoch 73 Batch 3 Loss 1.8558\n",
            "Epoch 73 Batch 4 Loss 1.8612\n",
            "Epoch 73 Batch 5 Loss 1.8655\n",
            "Epoch 73 Batch 6 Loss 1.8701\n",
            "Epoch 73 Batch 7 Loss 1.8678\n",
            "Epoch 73 Batch 8 Loss 1.8743\n",
            "Epoch 73 Batch 9 Loss 1.8710\n",
            "Epoch 73 Batch 10 Loss 1.8715\n",
            "Epoch 73 Batch 11 Loss 1.8728\n",
            "Epoch 73 Batch 12 Loss 1.8714\n",
            "Epoch 73 Batch 13 Loss 1.8669\n",
            "Epoch 73 Batch 14 Loss 1.8723\n",
            "Epoch 73 Batch 15 Loss 1.8688\n",
            "Epoch 73 Loss 1.8688\n",
            "Time taken for 1 epoch: 3.5958919525146484 secs\n",
            "\n",
            "Epoch 74 Batch 0 Loss 1.7006\n",
            "Epoch 74 Batch 1 Loss 1.6917\n",
            "Epoch 74 Batch 2 Loss 1.7217\n",
            "Epoch 74 Batch 3 Loss 1.7622\n",
            "Epoch 74 Batch 4 Loss 1.7596\n",
            "Epoch 74 Batch 5 Loss 1.7618\n",
            "Epoch 74 Batch 6 Loss 1.7647\n",
            "Epoch 74 Batch 7 Loss 1.7570\n",
            "Epoch 74 Batch 8 Loss 1.7621\n",
            "Epoch 74 Batch 9 Loss 1.7528\n",
            "Epoch 74 Batch 10 Loss 1.7510\n",
            "Epoch 74 Batch 11 Loss 1.7611\n",
            "Epoch 74 Batch 12 Loss 1.7669\n",
            "Epoch 74 Batch 13 Loss 1.7631\n",
            "Epoch 74 Batch 14 Loss 1.7646\n",
            "Epoch 74 Batch 15 Loss 1.7630\n",
            "Epoch 74 Loss 1.7630\n",
            "Time taken for 1 epoch: 3.566396951675415 secs\n",
            "\n",
            "Epoch 75 Batch 0 Loss 1.6213\n",
            "Epoch 75 Batch 1 Loss 1.6418\n",
            "Epoch 75 Batch 2 Loss 1.6511\n",
            "Epoch 75 Batch 3 Loss 1.6729\n",
            "Epoch 75 Batch 4 Loss 1.6695\n",
            "Epoch 75 Batch 5 Loss 1.6602\n",
            "Epoch 75 Batch 6 Loss 1.6545\n",
            "Epoch 75 Batch 7 Loss 1.6500\n",
            "Epoch 75 Batch 8 Loss 1.6465\n",
            "Epoch 75 Batch 9 Loss 1.6475\n",
            "Epoch 75 Batch 10 Loss 1.6543\n",
            "Epoch 75 Batch 11 Loss 1.6537\n",
            "Epoch 75 Batch 12 Loss 1.6616\n",
            "Epoch 75 Batch 13 Loss 1.6606\n",
            "Epoch 75 Batch 14 Loss 1.6626\n",
            "Epoch 75 Batch 15 Loss 1.6604\n",
            "Epoch 75 Loss 1.6604\n",
            "Time taken for 1 epoch: 3.6127471923828125 secs\n",
            "\n",
            "Epoch 76 Batch 0 Loss 1.6328\n",
            "Epoch 76 Batch 1 Loss 1.6454\n",
            "Epoch 76 Batch 2 Loss 1.5976\n",
            "Epoch 76 Batch 3 Loss 1.5976\n",
            "Epoch 76 Batch 4 Loss 1.5991\n",
            "Epoch 76 Batch 5 Loss 1.5858\n",
            "Epoch 76 Batch 6 Loss 1.5820\n",
            "Epoch 76 Batch 7 Loss 1.5870\n",
            "Epoch 76 Batch 8 Loss 1.5813\n",
            "Epoch 76 Batch 9 Loss 1.5663\n",
            "Epoch 76 Batch 10 Loss 1.5650\n",
            "Epoch 76 Batch 11 Loss 1.5742\n",
            "Epoch 76 Batch 12 Loss 1.5763\n",
            "Epoch 76 Batch 13 Loss 1.5794\n",
            "Epoch 76 Batch 14 Loss 1.5781\n",
            "Epoch 76 Batch 15 Loss 1.5717\n",
            "Epoch 76 Loss 1.5717\n",
            "Time taken for 1 epoch: 3.5938596725463867 secs\n",
            "\n",
            "Epoch 77 Batch 0 Loss 1.5245\n",
            "Epoch 77 Batch 1 Loss 1.5417\n",
            "Epoch 77 Batch 2 Loss 1.5292\n",
            "Epoch 77 Batch 3 Loss 1.5027\n",
            "Epoch 77 Batch 4 Loss 1.4733\n",
            "Epoch 77 Batch 5 Loss 1.4909\n",
            "Epoch 77 Batch 6 Loss 1.4924\n",
            "Epoch 77 Batch 7 Loss 1.4856\n",
            "Epoch 77 Batch 8 Loss 1.4861\n",
            "Epoch 77 Batch 9 Loss 1.4858\n",
            "Epoch 77 Batch 10 Loss 1.4817\n",
            "Epoch 77 Batch 11 Loss 1.4766\n",
            "Epoch 77 Batch 12 Loss 1.4724\n",
            "Epoch 77 Batch 13 Loss 1.4721\n",
            "Epoch 77 Batch 14 Loss 1.4717\n",
            "Epoch 77 Batch 15 Loss 1.4713\n",
            "Epoch 77 Loss 1.4713\n",
            "Time taken for 1 epoch: 3.604858160018921 secs\n",
            "\n",
            "Epoch 78 Batch 0 Loss 1.4136\n",
            "Epoch 78 Batch 1 Loss 1.3925\n",
            "Epoch 78 Batch 2 Loss 1.4178\n",
            "Epoch 78 Batch 3 Loss 1.4054\n",
            "Epoch 78 Batch 4 Loss 1.4015\n",
            "Epoch 78 Batch 5 Loss 1.3971\n",
            "Epoch 78 Batch 6 Loss 1.4006\n",
            "Epoch 78 Batch 7 Loss 1.3979\n",
            "Epoch 78 Batch 8 Loss 1.3966\n",
            "Epoch 78 Batch 9 Loss 1.3893\n",
            "Epoch 78 Batch 10 Loss 1.3804\n",
            "Epoch 78 Batch 11 Loss 1.3781\n",
            "Epoch 78 Batch 12 Loss 1.3853\n",
            "Epoch 78 Batch 13 Loss 1.3771\n",
            "Epoch 78 Batch 14 Loss 1.3783\n",
            "Epoch 78 Batch 15 Loss 1.3824\n",
            "Epoch 78 Loss 1.3824\n",
            "Time taken for 1 epoch: 3.5948305130004883 secs\n",
            "\n",
            "Epoch 79 Batch 0 Loss 1.2535\n",
            "Epoch 79 Batch 1 Loss 1.2527\n",
            "Epoch 79 Batch 2 Loss 1.2362\n",
            "Epoch 79 Batch 3 Loss 1.2754\n",
            "Epoch 79 Batch 4 Loss 1.2773\n",
            "Epoch 79 Batch 5 Loss 1.2910\n",
            "Epoch 79 Batch 6 Loss 1.2880\n",
            "Epoch 79 Batch 7 Loss 1.2885\n",
            "Epoch 79 Batch 8 Loss 1.2808\n",
            "Epoch 79 Batch 9 Loss 1.2797\n",
            "Epoch 79 Batch 10 Loss 1.2867\n",
            "Epoch 79 Batch 11 Loss 1.2869\n",
            "Epoch 79 Batch 12 Loss 1.2911\n",
            "Epoch 79 Batch 13 Loss 1.2886\n",
            "Epoch 79 Batch 14 Loss 1.2949\n",
            "Epoch 79 Batch 15 Loss 1.2897\n",
            "Epoch 79 Loss 1.2897\n",
            "Time taken for 1 epoch: 3.5984714031219482 secs\n",
            "\n",
            "Epoch 80 Batch 0 Loss 1.3002\n",
            "Epoch 80 Batch 1 Loss 1.2371\n",
            "Epoch 80 Batch 2 Loss 1.2372\n",
            "Epoch 80 Batch 3 Loss 1.2308\n",
            "Epoch 80 Batch 4 Loss 1.2325\n",
            "Epoch 80 Batch 5 Loss 1.2226\n",
            "Epoch 80 Batch 6 Loss 1.2138\n",
            "Epoch 80 Batch 7 Loss 1.2128\n",
            "Epoch 80 Batch 8 Loss 1.2150\n",
            "Epoch 80 Batch 9 Loss 1.2037\n",
            "Epoch 80 Batch 10 Loss 1.1996\n",
            "Epoch 80 Batch 11 Loss 1.1961\n",
            "Epoch 80 Batch 12 Loss 1.1929\n",
            "Epoch 80 Batch 13 Loss 1.1948\n",
            "Epoch 80 Batch 14 Loss 1.1933\n",
            "Epoch 80 Batch 15 Loss 1.1999\n",
            "Epoch 80 Loss 1.1999\n",
            "Time taken for 1 epoch: 3.60321044921875 secs\n",
            "\n",
            "Epoch 81 Batch 0 Loss 1.1807\n",
            "Epoch 81 Batch 1 Loss 1.1696\n",
            "Epoch 81 Batch 2 Loss 1.1206\n",
            "Epoch 81 Batch 3 Loss 1.1162\n",
            "Epoch 81 Batch 4 Loss 1.1036\n",
            "Epoch 81 Batch 5 Loss 1.1132\n",
            "Epoch 81 Batch 6 Loss 1.1117\n",
            "Epoch 81 Batch 7 Loss 1.1141\n",
            "Epoch 81 Batch 8 Loss 1.1075\n",
            "Epoch 81 Batch 9 Loss 1.1079\n",
            "Epoch 81 Batch 10 Loss 1.1074\n",
            "Epoch 81 Batch 11 Loss 1.1057\n",
            "Epoch 81 Batch 12 Loss 1.1076\n",
            "Epoch 81 Batch 13 Loss 1.1130\n",
            "Epoch 81 Batch 14 Loss 1.1132\n",
            "Epoch 81 Batch 15 Loss 1.1154\n",
            "Epoch 81 Loss 1.1154\n",
            "Time taken for 1 epoch: 3.584925651550293 secs\n",
            "\n",
            "Epoch 82 Batch 0 Loss 1.0278\n",
            "Epoch 82 Batch 1 Loss 0.9678\n",
            "Epoch 82 Batch 2 Loss 0.9747\n",
            "Epoch 82 Batch 3 Loss 0.9811\n",
            "Epoch 82 Batch 4 Loss 0.9977\n",
            "Epoch 82 Batch 5 Loss 1.0052\n",
            "Epoch 82 Batch 6 Loss 1.0229\n",
            "Epoch 82 Batch 7 Loss 1.0182\n",
            "Epoch 82 Batch 8 Loss 1.0140\n",
            "Epoch 82 Batch 9 Loss 1.0196\n",
            "Epoch 82 Batch 10 Loss 1.0281\n",
            "Epoch 82 Batch 11 Loss 1.0244\n",
            "Epoch 82 Batch 12 Loss 1.0264\n",
            "Epoch 82 Batch 13 Loss 1.0262\n",
            "Epoch 82 Batch 14 Loss 1.0295\n",
            "Epoch 82 Batch 15 Loss 1.0309\n",
            "Epoch 82 Loss 1.0309\n",
            "Time taken for 1 epoch: 3.591618776321411 secs\n",
            "\n",
            "Epoch 83 Batch 0 Loss 1.0050\n",
            "Epoch 83 Batch 1 Loss 1.0022\n",
            "Epoch 83 Batch 2 Loss 0.9981\n",
            "Epoch 83 Batch 3 Loss 1.0001\n",
            "Epoch 83 Batch 4 Loss 0.9828\n",
            "Epoch 83 Batch 5 Loss 0.9822\n",
            "Epoch 83 Batch 6 Loss 0.9772\n",
            "Epoch 83 Batch 7 Loss 0.9782\n",
            "Epoch 83 Batch 8 Loss 0.9777\n",
            "Epoch 83 Batch 9 Loss 0.9739\n",
            "Epoch 83 Batch 10 Loss 0.9659\n",
            "Epoch 83 Batch 11 Loss 0.9668\n",
            "Epoch 83 Batch 12 Loss 0.9678\n",
            "Epoch 83 Batch 13 Loss 0.9627\n",
            "Epoch 83 Batch 14 Loss 0.9579\n",
            "Epoch 83 Batch 15 Loss 0.9487\n",
            "Epoch 83 Loss 0.9487\n",
            "Time taken for 1 epoch: 3.612037181854248 secs\n",
            "\n",
            "Epoch 84 Batch 0 Loss 0.9056\n",
            "Epoch 84 Batch 1 Loss 0.9354\n",
            "Epoch 84 Batch 2 Loss 0.9196\n",
            "Epoch 84 Batch 3 Loss 0.9198\n",
            "Epoch 84 Batch 4 Loss 0.9113\n",
            "Epoch 84 Batch 5 Loss 0.8842\n",
            "Epoch 84 Batch 6 Loss 0.8828\n",
            "Epoch 84 Batch 7 Loss 0.8759\n",
            "Epoch 84 Batch 8 Loss 0.8710\n",
            "Epoch 84 Batch 9 Loss 0.8737\n",
            "Epoch 84 Batch 10 Loss 0.8732\n",
            "Epoch 84 Batch 11 Loss 0.8760\n",
            "Epoch 84 Batch 12 Loss 0.8710\n",
            "Epoch 84 Batch 13 Loss 0.8688\n",
            "Epoch 84 Batch 14 Loss 0.8670\n",
            "Epoch 84 Batch 15 Loss 0.8671\n",
            "Epoch 84 Loss 0.8671\n",
            "Time taken for 1 epoch: 3.6069018840789795 secs\n",
            "\n",
            "Epoch 85 Batch 0 Loss 0.7491\n",
            "Epoch 85 Batch 1 Loss 0.7471\n",
            "Epoch 85 Batch 2 Loss 0.7735\n",
            "Epoch 85 Batch 3 Loss 0.7852\n",
            "Epoch 85 Batch 4 Loss 0.7916\n",
            "Epoch 85 Batch 5 Loss 0.7908\n",
            "Epoch 85 Batch 6 Loss 0.7912\n",
            "Epoch 85 Batch 7 Loss 0.7980\n",
            "Epoch 85 Batch 8 Loss 0.7988\n",
            "Epoch 85 Batch 9 Loss 0.8037\n",
            "Epoch 85 Batch 10 Loss 0.8060\n",
            "Epoch 85 Batch 11 Loss 0.8017\n",
            "Epoch 85 Batch 12 Loss 0.7959\n",
            "Epoch 85 Batch 13 Loss 0.7967\n",
            "Epoch 85 Batch 14 Loss 0.7988\n",
            "Epoch 85 Batch 15 Loss 0.7943\n",
            "Epoch 85 Loss 0.7943\n",
            "Time taken for 1 epoch: 3.613368511199951 secs\n",
            "\n",
            "Epoch 86 Batch 0 Loss 0.7186\n",
            "Epoch 86 Batch 1 Loss 0.7353\n",
            "Epoch 86 Batch 2 Loss 0.7239\n",
            "Epoch 86 Batch 3 Loss 0.7438\n",
            "Epoch 86 Batch 4 Loss 0.7473\n",
            "Epoch 86 Batch 5 Loss 0.7473\n",
            "Epoch 86 Batch 6 Loss 0.7428\n",
            "Epoch 86 Batch 7 Loss 0.7406\n",
            "Epoch 86 Batch 8 Loss 0.7396\n",
            "Epoch 86 Batch 9 Loss 0.7361\n",
            "Epoch 86 Batch 10 Loss 0.7313\n",
            "Epoch 86 Batch 11 Loss 0.7298\n",
            "Epoch 86 Batch 12 Loss 0.7283\n",
            "Epoch 86 Batch 13 Loss 0.7295\n",
            "Epoch 86 Batch 14 Loss 0.7233\n",
            "Epoch 86 Batch 15 Loss 0.7208\n",
            "Epoch 86 Loss 0.7208\n",
            "Time taken for 1 epoch: 3.592900037765503 secs\n",
            "\n",
            "Epoch 87 Batch 0 Loss 0.6423\n",
            "Epoch 87 Batch 1 Loss 0.6670\n",
            "Epoch 87 Batch 2 Loss 0.6700\n",
            "Epoch 87 Batch 3 Loss 0.6729\n",
            "Epoch 87 Batch 4 Loss 0.6754\n",
            "Epoch 87 Batch 5 Loss 0.6696\n",
            "Epoch 87 Batch 6 Loss 0.6661\n",
            "Epoch 87 Batch 7 Loss 0.6684\n",
            "Epoch 87 Batch 8 Loss 0.6700\n",
            "Epoch 87 Batch 9 Loss 0.6625\n",
            "Epoch 87 Batch 10 Loss 0.6638\n",
            "Epoch 87 Batch 11 Loss 0.6621\n",
            "Epoch 87 Batch 12 Loss 0.6625\n",
            "Epoch 87 Batch 13 Loss 0.6617\n",
            "Epoch 87 Batch 14 Loss 0.6600\n",
            "Epoch 87 Batch 15 Loss 0.6574\n",
            "Epoch 87 Loss 0.6574\n",
            "Time taken for 1 epoch: 3.605956554412842 secs\n",
            "\n",
            "Epoch 88 Batch 0 Loss 0.6413\n",
            "Epoch 88 Batch 1 Loss 0.6027\n",
            "Epoch 88 Batch 2 Loss 0.6052\n",
            "Epoch 88 Batch 3 Loss 0.5985\n",
            "Epoch 88 Batch 4 Loss 0.6060\n",
            "Epoch 88 Batch 5 Loss 0.6063\n",
            "Epoch 88 Batch 6 Loss 0.6064\n",
            "Epoch 88 Batch 7 Loss 0.6038\n",
            "Epoch 88 Batch 8 Loss 0.6001\n",
            "Epoch 88 Batch 9 Loss 0.6014\n",
            "Epoch 88 Batch 10 Loss 0.6052\n",
            "Epoch 88 Batch 11 Loss 0.6048\n",
            "Epoch 88 Batch 12 Loss 0.6116\n",
            "Epoch 88 Batch 13 Loss 0.6088\n",
            "Epoch 88 Batch 14 Loss 0.6067\n",
            "Epoch 88 Batch 15 Loss 0.6024\n",
            "Epoch 88 Loss 0.6024\n",
            "Time taken for 1 epoch: 3.621208667755127 secs\n",
            "\n",
            "Epoch 89 Batch 0 Loss 0.5602\n",
            "Epoch 89 Batch 1 Loss 0.5343\n",
            "Epoch 89 Batch 2 Loss 0.5534\n",
            "Epoch 89 Batch 3 Loss 0.5535\n",
            "Epoch 89 Batch 4 Loss 0.5514\n",
            "Epoch 89 Batch 5 Loss 0.5459\n",
            "Epoch 89 Batch 6 Loss 0.5461\n",
            "Epoch 89 Batch 7 Loss 0.5414\n",
            "Epoch 89 Batch 8 Loss 0.5410\n",
            "Epoch 89 Batch 9 Loss 0.5403\n",
            "Epoch 89 Batch 10 Loss 0.5427\n",
            "Epoch 89 Batch 11 Loss 0.5422\n",
            "Epoch 89 Batch 12 Loss 0.5442\n",
            "Epoch 89 Batch 13 Loss 0.5424\n",
            "Epoch 89 Batch 14 Loss 0.5392\n",
            "Epoch 89 Batch 15 Loss 0.5386\n",
            "Epoch 89 Loss 0.5386\n",
            "Time taken for 1 epoch: 3.5886099338531494 secs\n",
            "\n",
            "Epoch 90 Batch 0 Loss 0.4865\n",
            "Epoch 90 Batch 1 Loss 0.4814\n",
            "Epoch 90 Batch 2 Loss 0.4860\n",
            "Epoch 90 Batch 3 Loss 0.4947\n",
            "Epoch 90 Batch 4 Loss 0.4829\n",
            "Epoch 90 Batch 5 Loss 0.4865\n",
            "Epoch 90 Batch 6 Loss 0.4834\n",
            "Epoch 90 Batch 7 Loss 0.4803\n",
            "Epoch 90 Batch 8 Loss 0.4819\n",
            "Epoch 90 Batch 9 Loss 0.4767\n",
            "Epoch 90 Batch 10 Loss 0.4787\n",
            "Epoch 90 Batch 11 Loss 0.4813\n",
            "Epoch 90 Batch 12 Loss 0.4820\n",
            "Epoch 90 Batch 13 Loss 0.4791\n",
            "Epoch 90 Batch 14 Loss 0.4808\n",
            "Epoch 90 Batch 15 Loss 0.4824\n",
            "Epoch 90 Loss 0.4824\n",
            "Time taken for 1 epoch: 3.598450183868408 secs\n",
            "\n",
            "Epoch 91 Batch 0 Loss 0.4658\n",
            "Epoch 91 Batch 1 Loss 0.4550\n",
            "Epoch 91 Batch 2 Loss 0.4351\n",
            "Epoch 91 Batch 3 Loss 0.4274\n",
            "Epoch 91 Batch 4 Loss 0.4389\n",
            "Epoch 91 Batch 5 Loss 0.4425\n",
            "Epoch 91 Batch 6 Loss 0.4421\n",
            "Epoch 91 Batch 7 Loss 0.4436\n",
            "Epoch 91 Batch 8 Loss 0.4440\n",
            "Epoch 91 Batch 9 Loss 0.4400\n",
            "Epoch 91 Batch 10 Loss 0.4405\n",
            "Epoch 91 Batch 11 Loss 0.4414\n",
            "Epoch 91 Batch 12 Loss 0.4394\n",
            "Epoch 91 Batch 13 Loss 0.4380\n",
            "Epoch 91 Batch 14 Loss 0.4363\n",
            "Epoch 91 Batch 15 Loss 0.4339\n",
            "Epoch 91 Loss 0.4339\n",
            "Time taken for 1 epoch: 3.595996618270874 secs\n",
            "\n",
            "Epoch 92 Batch 0 Loss 0.3819\n",
            "Epoch 92 Batch 1 Loss 0.3845\n",
            "Epoch 92 Batch 2 Loss 0.3805\n",
            "Epoch 92 Batch 3 Loss 0.3855\n",
            "Epoch 92 Batch 4 Loss 0.3913\n",
            "Epoch 92 Batch 5 Loss 0.3878\n",
            "Epoch 92 Batch 6 Loss 0.3835\n",
            "Epoch 92 Batch 7 Loss 0.3861\n",
            "Epoch 92 Batch 8 Loss 0.3853\n",
            "Epoch 92 Batch 9 Loss 0.3874\n",
            "Epoch 92 Batch 10 Loss 0.3863\n",
            "Epoch 92 Batch 11 Loss 0.3833\n",
            "Epoch 92 Batch 12 Loss 0.3858\n",
            "Epoch 92 Batch 13 Loss 0.3854\n",
            "Epoch 92 Batch 14 Loss 0.3842\n",
            "Epoch 92 Batch 15 Loss 0.3855\n",
            "Epoch 92 Loss 0.3855\n",
            "Time taken for 1 epoch: 3.6026790142059326 secs\n",
            "\n",
            "Epoch 93 Batch 0 Loss 0.3664\n",
            "Epoch 93 Batch 1 Loss 0.3513\n",
            "Epoch 93 Batch 2 Loss 0.3580\n",
            "Epoch 93 Batch 3 Loss 0.3547\n",
            "Epoch 93 Batch 4 Loss 0.3566\n",
            "Epoch 93 Batch 5 Loss 0.3577\n",
            "Epoch 93 Batch 6 Loss 0.3574\n",
            "Epoch 93 Batch 7 Loss 0.3539\n",
            "Epoch 93 Batch 8 Loss 0.3504\n",
            "Epoch 93 Batch 9 Loss 0.3483\n",
            "Epoch 93 Batch 10 Loss 0.3459\n",
            "Epoch 93 Batch 11 Loss 0.3435\n",
            "Epoch 93 Batch 12 Loss 0.3441\n",
            "Epoch 93 Batch 13 Loss 0.3433\n",
            "Epoch 93 Batch 14 Loss 0.3416\n",
            "Epoch 93 Batch 15 Loss 0.3416\n",
            "Epoch 93 Loss 0.3416\n",
            "Time taken for 1 epoch: 3.6027233600616455 secs\n",
            "\n",
            "Epoch 94 Batch 0 Loss 0.3407\n",
            "Epoch 94 Batch 1 Loss 0.3177\n",
            "Epoch 94 Batch 2 Loss 0.3144\n",
            "Epoch 94 Batch 3 Loss 0.3173\n",
            "Epoch 94 Batch 4 Loss 0.3174\n",
            "Epoch 94 Batch 5 Loss 0.3192\n",
            "Epoch 94 Batch 6 Loss 0.3131\n",
            "Epoch 94 Batch 7 Loss 0.3110\n",
            "Epoch 94 Batch 8 Loss 0.3121\n",
            "Epoch 94 Batch 9 Loss 0.3088\n",
            "Epoch 94 Batch 10 Loss 0.3096\n",
            "Epoch 94 Batch 11 Loss 0.3102\n",
            "Epoch 94 Batch 12 Loss 0.3100\n",
            "Epoch 94 Batch 13 Loss 0.3069\n",
            "Epoch 94 Batch 14 Loss 0.3056\n",
            "Epoch 94 Batch 15 Loss 0.3045\n",
            "Epoch 94 Loss 0.3045\n",
            "Time taken for 1 epoch: 3.5716893672943115 secs\n",
            "\n",
            "Epoch 95 Batch 0 Loss 0.2622\n",
            "Epoch 95 Batch 1 Loss 0.2719\n",
            "Epoch 95 Batch 2 Loss 0.2642\n",
            "Epoch 95 Batch 3 Loss 0.2825\n",
            "Epoch 95 Batch 4 Loss 0.2791\n",
            "Epoch 95 Batch 5 Loss 0.2790\n",
            "Epoch 95 Batch 6 Loss 0.2779\n",
            "Epoch 95 Batch 7 Loss 0.2748\n",
            "Epoch 95 Batch 8 Loss 0.2741\n",
            "Epoch 95 Batch 9 Loss 0.2732\n",
            "Epoch 95 Batch 10 Loss 0.2721\n",
            "Epoch 95 Batch 11 Loss 0.2717\n",
            "Epoch 95 Batch 12 Loss 0.2696\n",
            "Epoch 95 Batch 13 Loss 0.2692\n",
            "Epoch 95 Batch 14 Loss 0.2675\n",
            "Epoch 95 Batch 15 Loss 0.2665\n",
            "Epoch 95 Loss 0.2665\n",
            "Time taken for 1 epoch: 3.59903621673584 secs\n",
            "\n",
            "Epoch 96 Batch 0 Loss 0.2441\n",
            "Epoch 96 Batch 1 Loss 0.2392\n",
            "Epoch 96 Batch 2 Loss 0.2395\n",
            "Epoch 96 Batch 3 Loss 0.2413\n",
            "Epoch 96 Batch 4 Loss 0.2408\n",
            "Epoch 96 Batch 5 Loss 0.2407\n",
            "Epoch 96 Batch 6 Loss 0.2390\n",
            "Epoch 96 Batch 7 Loss 0.2386\n",
            "Epoch 96 Batch 8 Loss 0.2366\n",
            "Epoch 96 Batch 9 Loss 0.2361\n",
            "Epoch 96 Batch 10 Loss 0.2362\n",
            "Epoch 96 Batch 11 Loss 0.2367\n",
            "Epoch 96 Batch 12 Loss 0.2361\n",
            "Epoch 96 Batch 13 Loss 0.2368\n",
            "Epoch 96 Batch 14 Loss 0.2353\n",
            "Epoch 96 Batch 15 Loss 0.2361\n",
            "Epoch 96 Loss 0.2361\n",
            "Time taken for 1 epoch: 3.616533041000366 secs\n",
            "\n",
            "Epoch 97 Batch 0 Loss 0.2241\n",
            "Epoch 97 Batch 1 Loss 0.2212\n",
            "Epoch 97 Batch 2 Loss 0.2135\n",
            "Epoch 97 Batch 3 Loss 0.2143\n",
            "Epoch 97 Batch 4 Loss 0.2089\n",
            "Epoch 97 Batch 5 Loss 0.2105\n",
            "Epoch 97 Batch 6 Loss 0.2090\n",
            "Epoch 97 Batch 7 Loss 0.2067\n",
            "Epoch 97 Batch 8 Loss 0.2076\n",
            "Epoch 97 Batch 9 Loss 0.2072\n",
            "Epoch 97 Batch 10 Loss 0.2083\n",
            "Epoch 97 Batch 11 Loss 0.2095\n",
            "Epoch 97 Batch 12 Loss 0.2092\n",
            "Epoch 97 Batch 13 Loss 0.2084\n",
            "Epoch 97 Batch 14 Loss 0.2070\n",
            "Epoch 97 Batch 15 Loss 0.2071\n",
            "Epoch 97 Loss 0.2071\n",
            "Time taken for 1 epoch: 3.58730149269104 secs\n",
            "\n",
            "Epoch 98 Batch 0 Loss 0.1889\n",
            "Epoch 98 Batch 1 Loss 0.1850\n",
            "Epoch 98 Batch 2 Loss 0.1908\n",
            "Epoch 98 Batch 3 Loss 0.1868\n",
            "Epoch 98 Batch 4 Loss 0.1907\n",
            "Epoch 98 Batch 5 Loss 0.1911\n",
            "Epoch 98 Batch 6 Loss 0.1865\n",
            "Epoch 98 Batch 7 Loss 0.1840\n",
            "Epoch 98 Batch 8 Loss 0.1851\n",
            "Epoch 98 Batch 9 Loss 0.1846\n",
            "Epoch 98 Batch 10 Loss 0.1833\n",
            "Epoch 98 Batch 11 Loss 0.1814\n",
            "Epoch 98 Batch 12 Loss 0.1828\n",
            "Epoch 98 Batch 13 Loss 0.1824\n",
            "Epoch 98 Batch 14 Loss 0.1829\n",
            "Epoch 98 Batch 15 Loss 0.1838\n",
            "Epoch 98 Loss 0.1838\n",
            "Time taken for 1 epoch: 3.6082513332366943 secs\n",
            "\n",
            "Epoch 99 Batch 0 Loss 0.1519\n",
            "Epoch 99 Batch 1 Loss 0.1584\n",
            "Epoch 99 Batch 2 Loss 0.1598\n",
            "Epoch 99 Batch 3 Loss 0.1612\n",
            "Epoch 99 Batch 4 Loss 0.1594\n",
            "Epoch 99 Batch 5 Loss 0.1601\n",
            "Epoch 99 Batch 6 Loss 0.1585\n",
            "Epoch 99 Batch 7 Loss 0.1585\n",
            "Epoch 99 Batch 8 Loss 0.1597\n",
            "Epoch 99 Batch 9 Loss 0.1603\n",
            "Epoch 99 Batch 10 Loss 0.1618\n",
            "Epoch 99 Batch 11 Loss 0.1629\n",
            "Epoch 99 Batch 12 Loss 0.1624\n",
            "Epoch 99 Batch 13 Loss 0.1639\n",
            "Epoch 99 Batch 14 Loss 0.1626\n",
            "Epoch 99 Batch 15 Loss 0.1612\n",
            "Epoch 99 Loss 0.1612\n",
            "Time taken for 1 epoch: 3.604538679122925 secs\n",
            "\n",
            "Epoch 100 Batch 0 Loss 0.1408\n",
            "Epoch 100 Batch 1 Loss 0.1405\n",
            "Epoch 100 Batch 2 Loss 0.1432\n",
            "Epoch 100 Batch 3 Loss 0.1441\n",
            "Epoch 100 Batch 4 Loss 0.1419\n",
            "Epoch 100 Batch 5 Loss 0.1448\n",
            "Epoch 100 Batch 6 Loss 0.1506\n",
            "Epoch 100 Batch 7 Loss 0.1490\n",
            "Epoch 100 Batch 8 Loss 0.1489\n",
            "Epoch 100 Batch 9 Loss 0.1477\n",
            "Epoch 100 Batch 10 Loss 0.1463\n",
            "Epoch 100 Batch 11 Loss 0.1451\n",
            "Epoch 100 Batch 12 Loss 0.1450\n",
            "Epoch 100 Batch 13 Loss 0.1440\n",
            "Epoch 100 Batch 14 Loss 0.1444\n",
            "Epoch 100 Batch 15 Loss 0.1453\n",
            "Epoch 100 Loss 0.1453\n",
            "Time taken for 1 epoch: 3.6336543560028076 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-VP6qAOek6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm1hsVDxZPuj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = list(range(1,101))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbBoOpETaG4m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "59ae05a9-79c7-4be3-aee8-07ab9673af6e"
      },
      "source": [
        "plt.plot(epochs, los, 'g', label='Training-loss')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f623ed94a58>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVbEUCZagJ0G",
        "colab_type": "text"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMbqGTixu1cl",
        "colab_type": "text"
      },
      "source": [
        "#### Predicting one word at a time at the decoder and appending it to the output; then taking the complete sequence as an input to the decoder and repeating until maxlen or stop keyword appears"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5D5cv2Jd8-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(input_document):\n",
        "    input_document = document_tokenizer.texts_to_sequences([input_document])\n",
        "    input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "\n",
        "    encoder_input = tf.expand_dims(input_document[0], 0)\n",
        "\n",
        "    decoder_input = [summary_tokenizer.word_index[\"<go>\"]]\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "    for i in range(decoder_maxlen):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
        "\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input, \n",
        "            output,\n",
        "            False,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask\n",
        "        )\n",
        "\n",
        "        predictions = predictions[: ,-1:, :]\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        if predicted_id == summary_tokenizer.word_index[\"<stop>\"]:\n",
        "            return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0), attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkpdiW6wnmiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def summarize(input_document):\n",
        "    # not considering attention weights for now, can be used to plot attention heatmaps in the future\n",
        "    summarized = evaluate(input_document=input_document)[0].numpy()\n",
        "    summarized = np.expand_dims(summarized[1:], 0)  # not printing <go> token\n",
        "    return summary_tokenizer.sequences_to_texts(summarized)[0]  # since there is just one translated document"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNVOWPXFIn0k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "80fca095-b34b-454e-e05b-bb400df819bf"
      },
      "source": [
        "summarize(\n",
        "    \"పాకిస్తాన్‌ మాజీ క్రికెటర్‌ షోయబ్‌ అక్తర్‌ చైనీయులపై మండిపడ్డారు. ఏది పడితే అది తిని ప్రపంచాన్ని ప్రమాదంలోకి నెట్టారని ఆగ్రహం వ్యక్తం చేశారు. అసలు గబ్బిలాలు, కుక్కలు, పాములు, పిల్లులు, ఎలుకల్ని ఎలా తింటారని విస్మయం వ్యక్తం చేశారు. వాటి రక్తం, వ్యర్థాలను సైతం ఆహారంగా తీసుకునే చైనీయులపై కోపం వస్తోందని అన్నారు. కరోనా వ్యాప్తితో ప్రపంచంలోని అన్ని దేశాలు తీవ్ర ఇబ్బందులు ఎదుర్కొంటున్నాయని పేర్కొన్నారు. పర్యాటకం దెబ్బతిందని, ఆర్థిక వ్యవస్థ క్షీణించిందని  తెలిపారు. కోవిడ్‌ ప్రభావం క్రీడలపైనా పడిందని తన యూట్యూబ్‌ చానెల్‌లో చెప్పుకొచ్చారు. \"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'‘ఏది పడితే అది తిని ఈ మహమ్మారిని తెచ్చారు’'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNhCDi_6many",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b2229227-0aa2-46c9-8bb5-a860dfe1cf5e"
      },
      "source": [
        "summarize(\"దేశంలో జరిగే అన్ని ఫుట్‌బాల్‌ మ్యాచ్‌లను ఈ నెల 31 వరకు రద్దు చేస్తూ అఖిల భారత ఫుట్‌బాల్‌ సమాఖ్య (ఏఐఎఫ్‌ఎఫ్‌) శనివారం నిర్ణయం తీసుకుంది. దాంతో ఐ–లీగ్, డివిజన్‌–2, యూత్‌ లీగ్, గోల్డెన్‌ లీగ్, జాతీయ టోర్నీలు రద్దయ్యాయి. ఐ–లీగ్‌లోని 28 మ్యాచ్‌లను ప్రేక్షకులు లేకుండానే నిర్వహించాలని ఏఐఎఫ్‌ఎఫ్‌ తొలుత అనుకున్నా... కేంద్ర ఆరోగ్య మంత్రిత్వ శాఖ సలహా మేరకు ఈ నెల చివరి వరకు దేశంలో ఎటువంటి ఫుట్‌బాల్‌ మ్యాచ్‌లను నిర్వహించరాదని  నిర్ణయించింది.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'31 వరకు దేశంలో ‘నో’ ఫుట్\\u200cబాల్\\u200c'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfQs9BFHnEqE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9a1e56ae-9bc1-444b-96f6-a99a86a76b9e"
      },
      "source": [
        "summarize(\"కరోనావైరస్‌ మహమ్మారి కట్టడి కోసం లాక్‌డౌన్‌ విధించిన నేపథ్యంలో దేశీ స్మార్ట్‌ఫోన్స్‌ పరిశ్రమ తీవ్రంగా నష్టపోనుంది. ఇది సుమారు 2 బిలియన్‌ డాలర్ల మేర ఉండొచ్చని కౌంటర్‌పాయింట్‌ రీసెర్చ్‌ సంస్థ అంచనా వేసింది. మార్చి, ఏప్రిల్‌లో విక్రయాలు గణనీయంగా మందగించడం ఇందుకు కారణంగా ఉంటుందని పేర్కొంది. మార్చి మధ్య దాకా కరోనా మహమ్మారి ప్రభావం ఒక మోస్తరుగానే ఉన్నప్పటికీ.. ఆ తర్వాత విజృంభిస్తుండటంతో లాక్‌డౌన్‌ అనివార్యమైందని వివరించింది.దీని ఫలితంగా 2020లో స్మార్ట్‌ఫోన్ల విక్రయం గతేడాది నమోదైన 15.8 కోట్లతో పోలిస్తే 3 శాతం తగ్గి 15.3 కోట్లకు పరిమితం కావొచ్చని అంచనా వేసింది. వార్షిక ప్రాతిపదికన చూస్తే మార్చిలో 27 శాతం తగ్గనుండగా, ఏప్రిల్‌ 14 దాకా లాక్‌డౌన్‌ కొనసాగితే ఈ నెలలో దాదాపు 60 శాతం తగ్గుదల నమోదు కావొచ్చని కౌంటర్‌పాయింట్‌ రీసెర్చ్‌ అసోసియేట్‌ డైరెక్టర్‌ తరుణ్‌ పాఠక్‌ తెలిపారు. కరోనా వైరస్‌ మహమ్మారికి మూలకేంద్రమైన చైనా నుంచి విడిభాగాల సరఫరా దెబ్బతినడం వల్ల ఈ ఏడాది తొలి త్రైమాసికంలో స్మార్ట్‌ఫోన్‌ తయారీ సంస్థలు తీవ్రంగా ఇబ్బందిపడ్డాయి.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'నా శైలి అందరికీ తెలుసు'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2aDmNwonUQ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9d83baf6-72b8-42ee-aabb-31e208448916"
      },
      "source": [
        "summarize(\"కోవిడ్‌–19 కేసులపై డిపార్ట్‌మెంట్‌లో ఎవరూ మాట్లాడవద్దని ముఖ్యంగా మీడియాతో అసలు చర్చించవద్దని డీజీపీ మహేందర్‌రెడ్డి ఆదేశాలు జారీ చేశారు. గత 50 రోజులుగా లాక్‌డౌన్‌ కారణంగా.. జనసంచారం లేకపోవడం, అంతా ఇళ్లకే పరిమితమవడంతో రాష్ట్రంలో నేరాలు గణనీయంగా తగ్గుముఖం పట్టాయి. మర్కజ్‌ లింకులు, ఇక్కడి నుంచి వలస కూలీలను పంపడం, రాష్ట్రానికి వచ్చిన వలస కూలీల గుర్తింపు వరకు పోలీసులు అన్నీ తామై వ్యవహరించారు. కేంద్ర– రాష్ట్ర ప్రభుత్వాలు లాక్‌డౌన్‌కు మెజారిటీ ప్రాంతాల్లో మినహాయింపులు ఇచ్చాయి. మరోవైపు నేరాలు, దోపిడీలు, రోడ్డు ప్రమాదాలు, హత్యలు, దొంగతనాల కేసులు కూడా పెరుగుతున్నాయి. ఇకపై కరోనాతోపాటు సాధారణ నేరాల నియంత్రణకు కృషి చేయాలని డీజీపీ ఆదేశించారు.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\"యాంకర్\\u200c రవి 'తోటబావి' టీజర్\\u200c విడుదల\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ts2g_CAlnkhJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6658b63e-fe3c-4d06-d07a-0852450b74f0"
      },
      "source": [
        "summarize(\"అమెరికా అధ్యక్షుడు డొనాల్డ్‌ ట్రంప్‌ ఆడవారిపై అసభ్య వ్యాఖ్యలు చేసిన వీడియో అమెరికాలో కలకలం రేపింది. తరువాత అది నకిలీదని తేలింది. ఓ హాలీవుడ్‌ హీరోయిన్‌ పోర్న్‌ క్లిప్‌ ఇంటర్నెట్‌లో ప్రత్యక్షం.. అందులో ఉన్నది తాను కాదన్నా ఎవరూ నమ్మలేదు. కానీ, ఆమె చెప్పేది నిజమే. మనకు నచ్చిన సెలబ్రిటీల శరీరానికి సామాన్యుల ముఖాలను అంటించి మురిసిపోయే వీలున్న ఆర్టిఫిషియల్‌ ఇంటెలిజెన్స్‌ ఆధారంగా పనిచేసే ‘డీప్‌ఫేక్‌’సాఫ్ట్‌వేర్‌ సృష్టిస్తోన్న మాయాజాలమిది.ఈ యాప్‌ వచ్చిన కొత్తలో తమకు ఇష్టమైన హీరో, గాయకులు, రాజకీయ నాయకులను అనుకరిస్తూ.. పలు ఫొటోలు, వీడియోలు సృష్టించి, వాటిని సోషల్‌ మీడియా వేదికలపై పంచుకునేవారు. వాటికి వచ్చే లైకులు చూసి సంబరపడిపోయే వారు. అక్కడి వరకే పరిమితమైతే సరిపోయేది. కానీ, కొందరు మరో అడుగు ముందుకేసి.. సంచలనం సృష్టించాలని, తమ టీవీ చానళ్లకు రేటింగులను పెంచాలనే దురుద్దేశంతో డీప్‌ఫేక్‌ను వాడుకుని సెలబ్రిటీల ప్రతిష్టను దెబ్బతీసేలా తప్పుడు సందేశాలు, అసభ్య వీడియోలు సృష్టించి వాటిని వైరల్‌ చేస్తున్నారు. అవి నకిలీవని నిరూపించుకునేందుకు బాధితులు నానా తంటాలు పడుతున్నారు. \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'సెలబ్రిటీల ఫేక్\\u200c వీడియోలతో పోర్న్\\u200c క్లిప్\\u200cల తయారీ'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuS7yfDXnkrd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a8cc3d4a-767b-45b1-c51c-843690e0cf59"
      },
      "source": [
        "summarize(\"టిక్ టాక్  యాప్ ను భారతీయులు అధికంగా వినియోగిస్తున్నారు. రోజురోజుకు టిక్ టాక్  యాప్ ను ఉపయోగిస్తున్నవారి సంఖ్య పెరుగుతోంది. యూజర్లు తమ వీడియోల ద్వారా ప్రతిభను బయటపెడుతున్నారు. అదేవిధంగా చాలా మంది ఈ టిక్ టాక్  వీడియోల్లో డాన్స్ లు, పాటలు, చాలెంజ్ లు చేస్తూ.. సోషల్ మీడియాలో పాపులర్  అయిన విషయం తెలిసిందే. సాధారణంగా ఈ టిక్ టాక్  వీడియోలను చూస్తూ.. వైరల్ గా మారిన వీడియోలను షేర్  చేస్తూ యూజర్లు గంటల కొద్ది సమయాన్ని గడిపేస్తున్నారు. కొంతమందికి తమ ప్రతిభను బయట పెట్టడానికి.. మరికొంతమందికి కాలక్షేపం, వినోదానికి అనువుగా ఉండటంలో ఈ యాప్ పై యూజర్లు బోలడంత సమయాన్ని కేటాయిస్తున్నారు.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'భారతీయులు టిక్\\u200cటాక్\\u200cలో తెగ గడిపేశారు'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYlViKHwnk16",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f31aacba-9375-4e32-d0d7-ae7845752378"
      },
      "source": [
        "summarize(\"సూర్యుడికి సంబంధించిన అత్యంత అరుదైన ఫొటోలను అమెరికా ఖగోళ శాస్త్రవేత్తలు విడుదల చేశారు. ప్రపంచంలోనే అత్యంత పెద్దదైన సోలార్ టెలిస్కోప్ గా ప్రసిద్ధి పొందిన డేనియల్ కే ఇనౌయే సోలార్ టెలిస్కోప్ (డీకేఐఎస్ టీ) అద్భుత ఆవిష్కారానికి కారణమైంది. దీని ద్వారా సూర్యుడి ఉపరితలానికి సంబంధించిన అరుదైన ఫొటోలను చూసే అవకాశం మానవాళికి దక్కింది. కాగా హవాయి ద్వీపంలో ఏర్పాటు చేసిన ఈ భారీ టెలిస్కోపు ద్వారా సూర్యుడిని అత్యంత సమీపంగా చూస్తూ.. అంతర్గత శక్తిని అంచనా వేసే అవకాశం ఉంటుందని ఆస్ట్రోనాట్లు పేర్కొంటున్నారు. ప్రస్తుతం ఇది విడుదల చేసిన ఫొటోల ఆధారంగా.. సూర్యడి ఉపరితలం మీది కణాల వంటి ఆకారాలను జూమ్ చేయగా.. ఒక్కోటి అమెరికా రాష్ట్రం టెక్సాస్ పరిమాణంలో ఉందని తెలిపారు.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'ముందెన్నడూ చూడని సూర్యుడి అద్భుత ఫొటోలు'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGmAMAVYFu25",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "afdc80d0-359e-401e-9778-81204898c37e"
      },
      "source": [
        "summarize(\"అమెరికన్  ప్రెసిడెంట్ గా వ్యవహరిస్తున్న వ్యక్తి భార్యను ఫస్ట్  లేడీ అని సంబోధిస్తారు. హాలీవుడ్ లో ప్రస్తుతం ‘ఫస్ట్  లేడీస్ ’ అనే టైటిల్ తో ప్రెసిడెంట్  సతీమణులపై ఓ సిరీస్  రూపొందబోతోంది. ఈ సిరీస్  మొదటి సీజన్ లో అమెరికాకు ప్రెసిడెంట్లుగా వ్యవహరించిన ఇలియానోర్  రూజ్ వెల్ట్  , బెట్టీ ఫోర్డ్, ఒబామా భార్యల కథలను చర్చించనున్నారు. ఇందులో ఒబామా భార్య మిచ్చెలీ ఒబామా పాత్రలో వయోలా డేవిస్  నటించనున్నారు. ‘మిచ్చెలీ లాంటి ధైర్యవంతురాలు, ఎక్స్ ట్రార్డినరీ ఉమెన్  పాత్ర చేస్తున్నందుకు చాలా గర్వంగా ఉంది’ అని డేవిస్  అన్నారు.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'ఫస్ట్\\u200c లేడీ'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uue86MVeFvE1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3aae4d7d-a685-4f92-aa53-a558a1d5880c"
      },
      "source": [
        "summarize(\"రియల్  ఎస్టేట్  పోర్టల్  నోబ్రోకర్ .కామ్  హైదరాబాద్ లో అడుగుపెట్టింది. కస్టమర్  నుంచి కస్టమర్ కు సేవలందిస్తున్న ఈ కంపెనీ ఇప్పటికే అయిదు నగరాల్లో కార్యకలాపాలు సాగిస్తోంది. అద్దె, కొనుగోలు, విక్రయానికి ఉన్న రెసిడెన్షియల్, కమర్షియల్  ప్రాపర్టీస్ ను ఈ పోర్టల్ లో నమోదు చేయవచ్చు. రియల్  ఎస్టేట్  లావాదేవీల్లో ఎటువంటి బ్రోకరేజ్  వసూలు చేయబోమని కంపెనీ ఫౌండర్, సీబీవో సౌరభ్  గర్గ్  తెలిపారు. ఫౌండర్, సీటీవో అఖిల్  గుప్తాతో కలిసి బుధవారమిక్కడ మీడియాతో మాట్లాడారు. ‘ఒక్క భాగ్యనగరిలో బ్రోకరేజ్  వ్యాపారం ఏటా రూ.4,100 కోట్లుంది. ఈ నగరంలో నోబ్రోకర్ .కామ్ లో 15,000పైచిలుకు లిస్టింగ్స్  జరిగాయి. 33,000లకుపైగా కస్టమర్లు సేవలను వినియోగించుకున్నారు. దేశవ్యాప్తంగా సంస్థకు 65 లక్షల వినియోగదార్లున్నారు. 5 లక్షల లావాదేవీలు పూర్తి అయ్యాయి’ అని వివరించారు.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'హైదరాబాద్\\u200cలో నోబ్రోకర్\\u200c కామ్\\u200c సేవలు'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yf7CWdiFvSI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "78e8d1e3-e7f0-4b23-defc-86d673468a73"
      },
      "source": [
        "summarize(\"దక్షిణకొరియా దిగ్గజం శాంసంగ్  ఇండియాలో కొత్త స్మార్ట్ పోన్ ను లాంచ్  చేసింది. గెలాక్సీ ఏ 70కి కొనసాగింపుగా గెలాక్సీ ఏ 71ని ఆవిష్కరించింది. భారతదేశంలో శాంసంగ్ గెలాక్సీ ఏ 71. ప్రిజం క్రష్ బ్లాక్, ప్రిజం క్రష్ సిల్వర్, ప్రిజం క్రష్ బ్లూ కలర్ వేరియంట్ లలో వస్తుంది. ఫిబ్రవరి 24 నుండి శాంసంగ్ ఒపెరా హౌస్, శాంసంగ్.కామ్ తో పాటు ప్రముఖ ఆన్ లైన్ పోర్టల్ ల ద్వారా ఈ స్మార్ట్ ఫోన్ అందుబాటులో వుంటుంది.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'అద్భుత ఫీచర్లతో శాంసంగ్\\u200c గెలాక్సీ ఏ 71'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    }
  ]
}